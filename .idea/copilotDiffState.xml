<?xml version="1.0" encoding="UTF-8"?>
<project version="4">
  <component name="CopilotDiffPersistence">
    <option name="pendingDiffs">
      <map>
        <entry key="$PROJECT_DIR$/context-engine-mcp/Dockerfile">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/context-engine-mcp/Dockerfile" />
              <option name="originalContent" value="# Multi-stage build for Context Engine MCP&#10;&#10;# Stage 1: Build&#10;FROM maven:3.9-eclipse-temurin-17 AS builder&#10;&#10;WORKDIR /app&#10;&#10;# Copy pom.xml and download dependencies&#10;COPY pom.xml .&#10;RUN mvn dependency:go-offline -B&#10;&#10;# Copy source code&#10;COPY src ./src&#10;COPY .env.example .env.example&#10;&#10;# Build application&#10;RUN mvn clean package -DskipTests&#10;&#10;# Stage 2: Runtime&#10;FROM eclipse-temurin:17-jre-alpine&#10;&#10;# Install required packages&#10;RUN apk add --no-cache \&#10;    curl \&#10;    bash \&#10;    tzdata&#10;&#10;# Create app user&#10;RUN addgroup -g 1000 appuser &amp;&amp; \&#10;    adduser -D -u 1000 -G appuser appuser&#10;&#10;# Create necessary directories&#10;RUN mkdir -p /var/context-engine/offline-queue &amp;&amp; \&#10;    mkdir -p /var/context-engine/logs &amp;&amp; \&#10;    mkdir -p /opt/models &amp;&amp; \&#10;    chown -R appuser:appuser /var/context-engine /opt/models&#10;&#10;WORKDIR /app&#10;&#10;# Copy application&#10;COPY --from=builder /app/target/context-engine-mcp-*.jar app.jar&#10;COPY --from=builder /app/.env.example .env.example&#10;&#10;# Switch to non-root user&#10;USER appuser&#10;&#10;# Expose ports&#10;EXPOSE 8080 50051&#10;&#10;# Health check&#10;HEALTHCHECK --interval=30s --timeout=3s --start-period=40s --retries=3 \&#10;    CMD curl -f http://localhost:8080/actuator/health || exit 1&#10;&#10;# Set JVM options&#10;ENV JAVA_OPTS=&quot;-Xmx2g -Xms1g -XX:+UseG1GC -XX:MaxGCPauseMillis=200&quot;&#10;&#10;# Run application&#10;ENTRYPOINT [&quot;sh&quot;, &quot;-c&quot;, &quot;java $JAVA_OPTS -jar app.jar&quot;]" />
              <option name="updatedContent" value="# Multi-stage build for Context Engine MCP&#10;&#10;# Stage 1: Build&#10;FROM maven:3.9-eclipse-temurin-17 AS builder&#10;&#10;WORKDIR /app&#10;&#10;# Copy pom.xml and download dependencies&#10;COPY pom.xml .&#10;RUN mvn dependency:go-offline -B&#10;&#10;# Copy source code&#10;COPY src ./src&#10;COPY .env.example .env.example&#10;&#10;# Build application&#10;RUN mvn clean package -DskipTests&#10;&#10;# Stage 2: Runtime&#10;FROM eclipse-temurin:17-jre-alpine&#10;&#10;# Install required packages&#10;RUN apk add --no-cache \&#10;    curl \&#10;    bash \&#10;    tzdata&#10;&#10;# Create app user&#10;RUN addgroup -g 1000 appuser &amp;&amp; \&#10;    adduser -D -u 1000 -G appuser appuser&#10;&#10;# Create necessary directories&#10;RUN mkdir -p /var/context-engine/offline-queue &amp;&amp; \&#10;    mkdir -p /var/context-engine/logs &amp;&amp; \&#10;    mkdir -p /opt/models &amp;&amp; \&#10;    chown -R appuser:appuser /var/context-engine /opt/models&#10;&#10;WORKDIR /app&#10;&#10;# Copy application&#10;COPY --from=builder /app/target/context-engine-mcp-*.jar app.jar&#10;COPY --from=builder /app/.env.example .env.example&#10;&#10;# Switch to non-root user&#10;USER appuser&#10;&#10;# Expose ports&#10;EXPOSE 8080 50051&#10;&#10;# Health check&#10;HEALTHCHECK --interval=30s --timeout=10s --start-period=30s --retries=3 \&#10;  CMD curl -f http://localhost:8080/actuator/health || exit 1&#10;&#10;# Start the Spring Boot application&#10;CMD [&quot;java&quot;, &quot;-jar&quot;, &quot;app.jar&quot;]" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/context-engine-mcp/src/main/java/com/contextengine/mcp/adk/GoogleADKService.java">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/context-engine-mcp/src/main/java/com/contextengine/mcp/adk/GoogleADKService.java" />
              <option name="originalContent" value="package com.contextengine.mcp.adk;&#10;&#10;import com.google.cloud.vertexai.VertexAI;&#10;import com.google.cloud.aiplatform.v1.*;&#10;import com.google.cloud.aiplatform.v1.Value;&#10;import com.google.protobuf.Value.Builder;&#10;import org.springframework.beans.factory.annotation.Autowired;&#10;import com.contextengine.mcp.llm.*;&#10;import org.springframework.stereotype.Service;&#10;import org.slf4j.Logger;&#10;import org.slf4j.LoggerFactory;&#10;&#10;import jakarta.annotation.PostConstruct;&#10;import java.util.*;&#10;import java.util.concurrent.CompletableFuture;&#10;&#10;/**&#10; * Core Google ADK service that manages AI operations&#10; * Handles both local (Gemini Nano) and cloud (Vertex AI) deployments&#10; */&#10;@Service&#10;public class GoogleADKService {&#10;    private static final Logger logger = LoggerFactory.getLogger(GoogleADKService.class);&#10;    &#10;    @org.springframework.beans.factory.annotation.Value(&quot;${google.project.id}&quot;)&#10;    private String projectId;&#10;    &#10;    @org.springframework.beans.factory.annotation.Value(&quot;${google.location:us-central1}&quot;)&#10;    private String location;&#10;    &#10;    @org.springframework.beans.factory.annotation.Value(&quot;${deployment.mode:hybrid}&quot;)  // local, cloud, hybrid&#10;    private String deploymentMode;&#10;    &#10;    private VertexAI vertexAI;&#10;    private PredictionServiceClient predictionClient;&#10;    private IndexServiceClient indexClient;&#10;    private MatchServiceClient matchServiceClient;&#10;    &#10;    private final LLMRouter llmRouter;&#10;    private final ModelRegistry modelRegistry;&#10;    &#10;    public GoogleADKService(LLMRouter llmRouter, ModelRegistry modelRegistry) {&#10;        this.llmRouter = llmRouter;&#10;        this.modelRegistry = modelRegistry;&#10;    }&#10;    &#10;    @Autowired(required = false)&#10;    public void setVertexAI(VertexAI vertexAI) {&#10;        this.vertexAI = vertexAI;&#10;    }&#10;    &#10;    @Autowired(required = false)&#10;    public void setPredictionClient(PredictionServiceClient predictionClient) {&#10;        this.predictionClient = predictionClient;&#10;    }&#10;    &#10;    @Autowired(required = false)&#10;    public void setIndexClient(IndexServiceClient indexClient) {&#10;        this.indexClient = indexClient;&#10;    }&#10;    &#10;    @Autowired(required = false)&#10;    public void setMatchServiceClient(MatchServiceClient matchServiceClient) {&#10;        this.matchServiceClient = matchServiceClient;&#10;    }&#10;    &#10;    @PostConstruct&#10;    public void initialize() {&#10;        logger.info(&quot;Initializing Google ADK Service in {} mode&quot;, deploymentMode);&#10;        &#10;        try {&#10;            // Initialize Vertex AI if not injected&#10;            if (!deploymentMode.equals(&quot;local&quot;) &amp;&amp; vertexAI == null) {&#10;                vertexAI = new VertexAI(projectId, location);&#10;            }&#10;            &#10;            // Initialize clients if not injected&#10;            if (!deploymentMode.equals(&quot;local&quot;) &amp;&amp; predictionClient == null) {&#10;                predictionClient = PredictionServiceClient.create();&#10;            }&#10;            &#10;            if (!deploymentMode.equals(&quot;local&quot;) &amp;&amp; indexClient == null) {&#10;                indexClient = IndexServiceClient.create();&#10;            }&#10;            &#10;            if (!deploymentMode.equals(&quot;local&quot;) &amp;&amp; matchServiceClient == null) {&#10;                matchServiceClient = MatchServiceClient.create();&#10;            }&#10;            &#10;            // Initialize Vector Search for embeddings&#10;            if (!deploymentMode.equals(&quot;local&quot;) &amp;&amp; indexClient != null) {&#10;                initializeVectorSearch();&#10;                logger.info(&quot;Google ADK cloud services initialized&quot;);&#10;            }&#10;            &#10;            // Initialize local Gemini Nano if in local or hybrid mode&#10;            if (!deploymentMode.equals(&quot;cloud&quot;)) {&#10;                initializeLocalModels();&#10;            }&#10;            &#10;        } catch (Exception e) {&#10;            logger.error(&quot;Failed to initialize Google ADK services: {}&quot;, e.getMessage(), e);&#10;        }&#10;    }&#10;    &#10;    /**&#10;     * Initialize Vertex AI Vector Search for code embeddings&#10;     */&#10;    private void initializeVectorSearch() {&#10;        try {&#10;            // Create or get existing index for code embeddings&#10;            String indexId = &quot;code-embeddings-index&quot;;&#10;            IndexName indexName = IndexName.of(projectId, location, indexId);&#10;            &#10;            try {&#10;                Index existingIndex = indexClient.getIndex(indexName);&#10;                logger.info(&quot;Using existing vector search index: {}&quot;, existingIndex.getName());&#10;            } catch (Exception e) {&#10;                // Create new index if doesn't exist&#10;                Index.Builder indexBuilder = Index.newBuilder()&#10;                    .setDisplayName(&quot;Code Embeddings Index&quot;)&#10;                    .setDescription(&quot;Semantic search index for code&quot;)&#10;                    .setMetadataSchemaUri(&quot;gs://google-cloud-aiplatform/schema/matchingengine/metadata/nearest_neighbor_search_1.0.0.yaml&quot;);&#10;                &#10;                CreateIndexRequest createRequest = CreateIndexRequest.newBuilder()&#10;                    .setParent(LocationName.of(projectId, location).toString())&#10;                    .setIndex(indexBuilder.build())&#10;                    .build();&#10;                    &#10;                indexClient.createIndexAsync(createRequest);&#10;                logger.info(&quot;Created new vector search index&quot;);&#10;            }&#10;            &#10;        } catch (Exception e) {&#10;            logger.error(&quot;Failed to initialize vector search: {}&quot;, e.getMessage(), e);&#10;        }&#10;    }&#10;    &#10;    /**&#10;     * Initialize local models for hybrid deployment&#10;     */&#10;    private void initializeLocalModels() {&#10;        // Local models are initialized through the provider classes&#10;        logger.info(&quot;Local models configured for hybrid deployment&quot;);&#10;    }&#10;    &#10;    /**&#10;     * Search code using Vertex AI Vector Search&#10;     */&#10;    public CompletableFuture&lt;SearchResult&gt; searchCode(String query, String projectId, SearchOptions options) {&#10;        return CompletableFuture.supplyAsync(() -&gt; {&#10;            try {&#10;                // First, generate embedding for the query using Gemini&#10;                LLMRequest embeddingRequest = LLMRequest.builder()&#10;                    .operation(&quot;embed&quot;)&#10;                    .embeddingRequest(new EmbeddingRequest(query, 768))&#10;                    .requiredCapabilities(Set.of(ModelCapability.BATCH_PROCESSING))&#10;                    .build();&#10;                &#10;                LLMResponse embeddingResponse = llmRouter.route(embeddingRequest).get();&#10;                float[] queryEmbedding = embeddingResponse.asEmbedding().getEmbedding();&#10;                &#10;                // Search in Vector Search&#10;                FindNeighborsRequest searchRequest = FindNeighborsRequest.newBuilder()&#10;                    .setIndexEndpoint(getIndexEndpoint(projectId))&#10;                    .setDeployedIndexId(getDeployedIndexId(projectId))&#10;                    .addQueries(FindNeighborsRequest.Query.newBuilder()&#10;                        .setNeighborCount(options.getLimit())&#10;                        .addAllEmbedding(floatArrayToList(queryEmbedding))&#10;                        .build())&#10;                    .build();&#10;                &#10;                FindNeighborsResponse response = matchServiceClient.findNeighbors(searchRequest);&#10;                &#10;                // Convert to search results&#10;                List&lt;CodeSearchResult&gt; results = new ArrayList&lt;&gt;();&#10;                for (FindNeighborsResponse.Neighbor neighbor : response.getNearestNeighbors(0).getNeighborsList()) {&#10;                    results.add(new CodeSearchResult(&#10;                        neighbor.getDatapoint().getDatapointId(),&#10;                        neighbor.getDistance(),&#10;                        extractMetadata(neighbor.getDatapoint())&#10;                    ));&#10;                }&#10;                &#10;                return new SearchResult(results, query, embeddingResponse.getModelUsed());&#10;                &#10;            } catch (Exception e) {&#10;                logger.error(&quot;Search failed: {}&quot;, e.getMessage(), e);&#10;                throw new RuntimeException(&quot;Code search failed&quot;, e);&#10;            }&#10;        });&#10;    }&#10;    &#10;    /**&#10;     * Optimize context using Google ADK models&#10;     */&#10;    public CompletableFuture&lt;OptimizedContext&gt; optimizeContext(String filePath, String taskType, OptimizationProfile profile) {&#10;        return CompletableFuture.supplyAsync(() -&gt; {&#10;            try {&#10;                // Read file content&#10;                String content = readFileContent(filePath);&#10;                &#10;                // Determine optimization strategy based on task type&#10;                String systemPrompt = buildOptimizationPrompt(taskType, profile);&#10;                &#10;                // Stage 1: Use Gemini Flash for initial optimization&#10;                CompletionRequest stage1Request = CompletionRequest.builder()&#10;                    .prompt(content)&#10;                    .systemPrompt(systemPrompt)&#10;                    .maxTokens(8192)&#10;                    .temperature(0.1)&#10;                    .build();&#10;                &#10;                LLMRequest llmRequest = LLMRequest.builder()&#10;                    .operation(&quot;optimize&quot;)&#10;                    .completionRequest(stage1Request)&#10;                    .requiredCapabilities(Set.of(ModelCapability.CODE_OPTIMIZATION))&#10;                    .build();&#10;                &#10;                LLMResponse response = llmRouter.route(llmRequest).get();&#10;                String optimizedContent = response.asCompletion().getContent();&#10;                &#10;                // Stage 2: Validate if confidence is high enough&#10;                double confidence = calculateOptimizationConfidence(content, optimizedContent);&#10;                &#10;                if (confidence &lt; 0.85) {&#10;                    // Use better model for validation&#10;                    logger.info(&quot;Low confidence ({}), escalating to better model&quot;, confidence);&#10;                    llmRequest = LLMRequest.builder()&#10;                        .operation(&quot;optimize&quot;)&#10;                        .preferredModel(&quot;gemini-pro&quot;)&#10;                        .completionRequest(stage1Request)&#10;                        .build();&#10;                        &#10;                    response = llmRouter.route(llmRequest).get();&#10;                    optimizedContent = response.asCompletion().getContent();&#10;                }&#10;                &#10;                // Calculate metrics&#10;                int originalTokens = estimateTokens(content);&#10;                int optimizedTokens = estimateTokens(optimizedContent);&#10;                double reduction = 1.0 - ((double) optimizedTokens / originalTokens);&#10;                &#10;                return new OptimizedContext(&#10;                    filePath,&#10;                    optimizedContent,&#10;                    originalTokens,&#10;                    optimizedTokens,&#10;                    reduction,&#10;                    confidence,&#10;                    response.getModelUsed()&#10;                );&#10;                &#10;            } catch (Exception e) {&#10;                logger.error(&quot;Context optimization failed: {}&quot;, e.getMessage(), e);&#10;                throw new RuntimeException(&quot;Context optimization failed&quot;, e);&#10;            }&#10;        });&#10;    }&#10;    &#10;    /**&#10;     * Store embeddings in Vertex AI Vector Search&#10;     */&#10;    public CompletableFuture&lt;Void&gt; storeEmbeddings(String projectId, String fileId, float[] embeddings, Map&lt;String, String&gt; metadata) {&#10;        return CompletableFuture.runAsync(() -&gt; {&#10;            try {&#10;                // Create datapoint&#10;                IndexDatapoint datapoint = IndexDatapoint.newBuilder()&#10;                    .setDatapointId(fileId)&#10;                    .addAllEmbedding(floatArrayToList(embeddings))&#10;                    .putAllMetadata(convertMetadata(metadata))&#10;                    .build();&#10;                &#10;                // Upsert to index&#10;                UpsertDatapointsRequest request = UpsertDatapointsRequest.newBuilder()&#10;                    .setIndex(getIndexName(projectId))&#10;                    .addDatapoints(datapoint)&#10;                    .build();&#10;                    &#10;                indexClient.upsertDatapoints(request);&#10;                logger.debug(&quot;Stored embeddings for file: {}&quot;, fileId);&#10;                &#10;            } catch (Exception e) {&#10;                logger.error(&quot;Failed to store embeddings: {}&quot;, e.getMessage(), e);&#10;                throw new RuntimeException(&quot;Failed to store embeddings&quot;, e);&#10;            }&#10;        });&#10;    }&#10;    &#10;    /**&#10;     * Batch process files using Vertex AI Pipelines&#10;     */&#10;    public CompletableFuture&lt;BatchProcessResult&gt; batchProcessFiles(List&lt;String&gt; filePaths, String operation) {&#10;        return CompletableFuture.supplyAsync(() -&gt; {&#10;            try {&#10;                // Create batch prediction job&#10;                BatchPredictionJob.Builder jobBuilder = BatchPredictionJob.newBuilder()&#10;                    .setDisplayName(&quot;Batch &quot; + operation + &quot; - &quot; + System.currentTimeMillis())&#10;                    .setModel(getModelResourceName(operation))&#10;                    .setInputConfig(BatchPredictionJob.InputConfig.newBuilder()&#10;                        .setInstancesFormat(&quot;jsonl&quot;)&#10;                        .setGcsSource(createBatchInput(filePaths))&#10;                        .build())&#10;                    .setOutputConfig(BatchPredictionJob.OutputConfig.newBuilder()&#10;                        .setPredictionsFormat(&quot;jsonl&quot;)&#10;                        .setGcsDestination(createBatchOutput())&#10;                        .build());&#10;                &#10;                // Submit job&#10;                CreateBatchPredictionJobRequest request = CreateBatchPredictionJobRequest.newBuilder()&#10;                    .setParent(LocationName.of(projectId, location).toString())&#10;                    .setBatchPredictionJob(jobBuilder.build())&#10;                    .build();&#10;                    &#10;                predictionClient.createBatchPredictionJob(request);&#10;                &#10;                return new BatchProcessResult(&#10;                    job.getName(),&#10;                    filePaths.size(),&#10;                    operation,&#10;                    &quot;SUBMITTED&quot;&#10;                );&#10;                &#10;            } catch (Exception e) {&#10;                logger.error(&quot;Batch processing failed: {}&quot;, e.getMessage(), e);&#10;                throw new RuntimeException(&quot;Batch processing failed&quot;, e);&#10;            }&#10;        });&#10;    }&#10;    &#10;    // Helper methods&#10;    &#10;    private String buildOptimizationPrompt(String taskType, OptimizationProfile profile) {&#10;        return String.format(&#10;            &quot;Optimize this code for %s. Remove boilerplate, preserve functionality. Profile: %s&quot;,&#10;            taskType != null ? taskType : &quot;general&quot;,&#10;            profile != null ? profile.name() : &quot;BALANCED&quot;&#10;        );&#10;    }&#10;    &#10;    private double calculateOptimizationConfidence(String original, String optimized) {&#10;        // Simple confidence calculation based on preservation of key elements&#10;        // In production, use more sophisticated AST-based analysis&#10;        int originalLines = original.split(&quot;\n&quot;).length;&#10;        int optimizedLines = optimized.split(&quot;\n&quot;).length;&#10;        double reductionRatio = (double) optimizedLines / originalLines;&#10;        &#10;        // Too much reduction might indicate loss of important code&#10;        if (reductionRatio &lt; 0.3) return 0.7;&#10;        if (reductionRatio &gt; 0.9) return 0.6;&#10;        return 0.9;&#10;    }&#10;    &#10;    private int estimateTokens(String text) {&#10;        // Rough estimation: 1 token \u2248 4 characters&#10;        return text.length() / 4;&#10;    }&#10;    &#10;    private List&lt;Float&gt; floatArrayToList(float[] array) {&#10;        List&lt;Float&gt; list = new ArrayList&lt;&gt;(array.length);&#10;        for (float f : array) {&#10;            list.add(f);&#10;        }&#10;        return list;&#10;    }&#10;    &#10;    private String readFileContent(String filePath) {&#10;        // Implement file reading&#10;        return &quot;&quot;;&#10;    }&#10;    &#10;    private String getIndexEndpoint(String projectId) {&#10;        return String.format(&quot;projects/%s/locations/%s/indexEndpoints/code-search&quot;, projectId, location);&#10;    }&#10;    &#10;    private String getDeployedIndexId(String projectId) {&#10;        return &quot;deployed-code-index&quot;;&#10;    }&#10;    &#10;    private String getIndexName(String projectId) {&#10;        return IndexName.of(projectId, location, &quot;code-embeddings-index&quot;).toString();&#10;    }&#10;    &#10;    private String getModelResourceName(String operation) {&#10;        // Map operation to model endpoint&#10;        return String.format(&quot;projects/%s/locations/%s/models/gemini-flash&quot;, projectId, location);&#10;    }&#10;    &#10;    private Map&lt;String, Value&gt; convertMetadata(Map&lt;String, String&gt; metadata) {&#10;        Map&lt;String, Value&gt; result = new HashMap&lt;&gt;();&#10;        metadata.forEach((k, v) -&gt; {&#10;            Builder builder = com.google.protobuf.Value.newBuilder();&#10;            result.put(k, builder.setStringValue(v).build());&#10;        });&#10;        return result;&#10;    }&#10;    &#10;    private Map&lt;String, String&gt; extractMetadata(IndexDatapoint datapoint) {&#10;        Map&lt;String, String&gt; result = new HashMap&lt;&gt;();&#10;        datapoint.getMetadataMap().forEach((k, v) -&gt; result.put(k, v.getStringValue()));&#10;        return result;&#10;    }&#10;    &#10;    private GcsSource createBatchInput(List&lt;String&gt; filePaths) {&#10;        // Create GCS input for batch processing&#10;        return GcsSource.newBuilder()&#10;            .addUris(&quot;gs://context-engine-batch/input/&quot; + System.currentTimeMillis() + &quot;.jsonl&quot;)&#10;            .build();&#10;    }&#10;    &#10;    private GcsDestination createBatchOutput() {&#10;        // Create GCS output for batch processing&#10;        return GcsDestination.newBuilder()&#10;            .setOutputUriPrefix(&quot;gs://context-engine-batch/output/&quot; + System.currentTimeMillis())&#10;            .build();&#10;    }&#10;}&#10;" />
              <option name="updatedContent" value="package com.contextengine.mcp.adk;&#10;&#10;import com.google.cloud.vertexai.VertexAI;&#10;import com.google.cloud.aiplatform.v1.*;&#10;import com.google.cloud.aiplatform.v1.Value;&#10;import com.google.protobuf.Value.Builder;&#10;import org.springframework.beans.factory.annotation.Autowired;&#10;import com.contextengine.mcp.llm.*;&#10;import org.springframework.stereotype.Service;&#10;import org.slf4j.Logger;&#10;import org.slf4j.LoggerFactory;&#10;&#10;import jakarta.annotation.PostConstruct;&#10;import java.util.*;&#10;import java.util.concurrent.CompletableFuture;&#10;&#10;/**&#10; * Core Google ADK service that manages AI operations&#10; * Handles both local (Gemini Nano) and cloud (Vertex AI) deployments&#10; */&#10;@Service&#10;public class GoogleADKService {&#10;    private static final Logger logger = LoggerFactory.getLogger(GoogleADKService.class);&#10;    &#10;    @org.springframework.beans.factory.annotation.Value(&quot;${google.project.id}&quot;)&#10;    private String projectId;&#10;    &#10;    @org.springframework.beans.factory.annotation.Value(&quot;${google.location:us-central1}&quot;)&#10;    private String location;&#10;    &#10;    @org.springframework.beans.factory.annotation.Value(&quot;${deployment.mode:hybrid}&quot;)  // local, cloud, hybrid&#10;    private String deploymentMode;&#10;    &#10;    private VertexAI vertexAI;&#10;    private PredictionServiceClient predictionClient;&#10;    private IndexServiceClient indexClient;&#10;    private MatchServiceClient matchServiceClient;&#10;    &#10;    private final LLMRouter llmRouter;&#10;    private final ModelRegistry modelRegistry;&#10;    &#10;    public GoogleADKService(LLMRouter llmRouter, ModelRegistry modelRegistry) {&#10;        this.llmRouter = llmRouter;&#10;        this.modelRegistry = modelRegistry;&#10;    }&#10;    &#10;    @Autowired(required = false)&#10;    public void setVertexAI(VertexAI vertexAI) {&#10;        this.vertexAI = vertexAI;&#10;    }&#10;    &#10;    @Autowired(required = false)&#10;    public void setPredictionClient(PredictionServiceClient predictionClient) {&#10;        this.predictionClient = predictionClient;&#10;    }&#10;    &#10;    @Autowired(required = false)&#10;    public void setIndexClient(IndexServiceClient indexClient) {&#10;        this.indexClient = indexClient;&#10;    }&#10;    &#10;    @Autowired(required = false)&#10;    public void setMatchServiceClient(MatchServiceClient matchServiceClient) {&#10;        this.matchServiceClient = matchServiceClient;&#10;    }&#10;    &#10;    /**&#10;     * Initializes the Google ADK Service, setting up cloud and local resources as needed.&#10;     * Logs errors and warnings for missing configuration or failures.&#10;     */&#10;    @PostConstruct&#10;    public void initialize() {&#10;        logger.info(&quot;Initializing Google ADK Service in {} mode&quot;, deploymentMode);&#10;        if (projectId == null || projectId.isEmpty()) {&#10;            logger.warn(&quot;GOOGLE_PROJECT_ID is not set. Cloud features may not work properly.&quot;);&#10;        }&#10;        if (location == null || location.isEmpty()) {&#10;            logger.warn(&quot;GOOGLE_LOCATION is not set. Defaulting to us-central1.&quot;);&#10;        }&#10;        try {&#10;            // Initialize Vertex AI if not injected&#10;            if (!deploymentMode.equals(&quot;local&quot;) &amp;&amp; vertexAI == null) {&#10;                vertexAI = new VertexAI(projectId, location);&#10;            }&#10;            // Initialize clients if not injected&#10;            if (!deploymentMode.equals(&quot;local&quot;) &amp;&amp; predictionClient == null) {&#10;                predictionClient = PredictionServiceClient.create();&#10;            }&#10;            if (!deploymentMode.equals(&quot;local&quot;) &amp;&amp; indexClient == null) {&#10;                indexClient = IndexServiceClient.create();&#10;            }&#10;            if (!deploymentMode.equals(&quot;local&quot;) &amp;&amp; matchServiceClient == null) {&#10;                matchServiceClient = MatchServiceClient.create();&#10;            }&#10;            // Initialize Vector Search for embeddings&#10;            if (!deploymentMode.equals(&quot;local&quot;) &amp;&amp; indexClient != null) {&#10;                initializeVectorSearch();&#10;                logger.info(&quot;Google ADK cloud services initialized&quot;);&#10;            }&#10;            // Initialize local Gemini Nano if in local or hybrid mode&#10;            if (!deploymentMode.equals(&quot;cloud&quot;)) {&#10;                initializeLocalModels();&#10;            }&#10;        } catch (Exception e) {&#10;            logger.error(&quot;Failed to initialize Google ADK Service&quot;, e);&#10;        }&#10;    }&#10;    &#10;    /**&#10;     * Initialize Vertex AI Vector Search for code embeddings&#10;     */&#10;    private void initializeVectorSearch() {&#10;        try {&#10;            // Create or get existing index for code embeddings&#10;            String indexId = &quot;code-embeddings-index&quot;;&#10;            IndexName indexName = IndexName.of(projectId, location, indexId);&#10;            &#10;            try {&#10;                Index existingIndex = indexClient.getIndex(indexName);&#10;                logger.info(&quot;Using existing vector search index: {}&quot;, existingIndex.getName());&#10;            } catch (Exception e) {&#10;                // Create new index if doesn't exist&#10;                Index.Builder indexBuilder = Index.newBuilder()&#10;                    .setDisplayName(&quot;Code Embeddings Index&quot;)&#10;                    .setDescription(&quot;Semantic search index for code&quot;)&#10;                    .setMetadataSchemaUri(&quot;gs://google-cloud-aiplatform/schema/matchingengine/metadata/nearest_neighbor_search_1.0.0.yaml&quot;);&#10;                &#10;                CreateIndexRequest createRequest = CreateIndexRequest.newBuilder()&#10;                    .setParent(LocationName.of(projectId, location).toString())&#10;                    .setIndex(indexBuilder.build())&#10;                    .build();&#10;                    &#10;                indexClient.createIndexAsync(createRequest);&#10;                logger.info(&quot;Created new vector search index&quot;);&#10;            }&#10;            &#10;        } catch (Exception e) {&#10;            logger.error(&quot;Failed to initialize vector search: {}&quot;, e.getMessage(), e);&#10;        }&#10;    }&#10;    &#10;    /**&#10;     * Initialize local models for hybrid deployment&#10;     */&#10;    private void initializeLocalModels() {&#10;        // Local models are initialized through the provider classes&#10;        logger.info(&quot;Local models configured for hybrid deployment&quot;);&#10;    }&#10;    &#10;    /**&#10;     * Search code using Vertex AI Vector Search&#10;     */&#10;    public CompletableFuture&lt;SearchResult&gt; searchCode(String query, String projectId, SearchOptions options) {&#10;        return CompletableFuture.supplyAsync(() -&gt; {&#10;            try {&#10;                // First, generate embedding for the query using Gemini&#10;                LLMRequest embeddingRequest = LLMRequest.builder()&#10;                    .operation(&quot;embed&quot;)&#10;                    .embeddingRequest(new EmbeddingRequest(query, 768))&#10;                    .requiredCapabilities(Set.of(ModelCapability.BATCH_PROCESSING))&#10;                    .build();&#10;                &#10;                LLMResponse embeddingResponse = llmRouter.route(embeddingRequest).get();&#10;                float[] queryEmbedding = embeddingResponse.asEmbedding().getEmbedding();&#10;                &#10;                // Search in Vector Search&#10;                FindNeighborsRequest searchRequest = FindNeighborsRequest.newBuilder()&#10;                    .setIndexEndpoint(getIndexEndpoint(projectId))&#10;                    .setDeployedIndexId(getDeployedIndexId(projectId))&#10;                    .addQueries(FindNeighborsRequest.Query.newBuilder()&#10;                        .setNeighborCount(options.getLimit())&#10;                        .addAllEmbedding(floatArrayToList(queryEmbedding))&#10;                        .build())&#10;                    .build();&#10;                &#10;                FindNeighborsResponse response = matchServiceClient.findNeighbors(searchRequest);&#10;                &#10;                // Convert to search results&#10;                List&lt;CodeSearchResult&gt; results = new ArrayList&lt;&gt;();&#10;                for (FindNeighborsResponse.Neighbor neighbor : response.getNearestNeighbors(0).getNeighborsList()) {&#10;                    results.add(new CodeSearchResult(&#10;                        neighbor.getDatapoint().getDatapointId(),&#10;                        neighbor.getDistance(),&#10;                        extractMetadata(neighbor.getDatapoint())&#10;                    ));&#10;                }&#10;                &#10;                return new SearchResult(results, query, embeddingResponse.getModelUsed());&#10;                &#10;            } catch (Exception e) {&#10;                logger.error(&quot;Search failed: {}&quot;, e.getMessage(), e);&#10;                throw new RuntimeException(&quot;Code search failed&quot;, e);&#10;            }&#10;        });&#10;    }&#10;    &#10;    /**&#10;     * Optimize context using Google ADK models&#10;     */&#10;    public CompletableFuture&lt;OptimizedContext&gt; optimizeContext(String filePath, String taskType, OptimizationProfile profile) {&#10;        return CompletableFuture.supplyAsync(() -&gt; {&#10;            try {&#10;                // Read file content&#10;                String content = readFileContent(filePath);&#10;                &#10;                // Determine optimization strategy based on task type&#10;                String systemPrompt = buildOptimizationPrompt(taskType, profile);&#10;                &#10;                // Stage 1: Use Gemini Flash for initial optimization&#10;                CompletionRequest stage1Request = CompletionRequest.builder()&#10;                    .prompt(content)&#10;                    .systemPrompt(systemPrompt)&#10;                    .maxTokens(8192)&#10;                    .temperature(0.1)&#10;                    .build();&#10;                &#10;                LLMRequest llmRequest = LLMRequest.builder()&#10;                    .operation(&quot;optimize&quot;)&#10;                    .completionRequest(stage1Request)&#10;                    .requiredCapabilities(Set.of(ModelCapability.CODE_OPTIMIZATION))&#10;                    .build();&#10;                &#10;                LLMResponse response = llmRouter.route(llmRequest).get();&#10;                String optimizedContent = response.asCompletion().getContent();&#10;                &#10;                // Stage 2: Validate if confidence is high enough&#10;                double confidence = calculateOptimizationConfidence(content, optimizedContent);&#10;                &#10;                if (confidence &lt; 0.85) {&#10;                    // Use better model for validation&#10;                    logger.info(&quot;Low confidence ({}), escalating to better model&quot;, confidence);&#10;                    llmRequest = LLMRequest.builder()&#10;                        .operation(&quot;optimize&quot;)&#10;                        .preferredModel(&quot;gemini-pro&quot;)&#10;                        .completionRequest(stage1Request)&#10;                        .build();&#10;                        &#10;                    response = llmRouter.route(llmRequest).get();&#10;                    optimizedContent = response.asCompletion().getContent();&#10;                }&#10;                &#10;                // Calculate metrics&#10;                int originalTokens = estimateTokens(content);&#10;                int optimizedTokens = estimateTokens(optimizedContent);&#10;                double reduction = 1.0 - ((double) optimizedTokens / originalTokens);&#10;                &#10;                return new OptimizedContext(&#10;                    filePath,&#10;                    optimizedContent,&#10;                    originalTokens,&#10;                    optimizedTokens,&#10;                    reduction,&#10;                    confidence,&#10;                    response.getModelUsed()&#10;                );&#10;                &#10;            } catch (Exception e) {&#10;                logger.error(&quot;Context optimization failed: {}&quot;, e.getMessage(), e);&#10;                throw new RuntimeException(&quot;Context optimization failed&quot;, e);&#10;            }&#10;        });&#10;    }&#10;    &#10;    /**&#10;     * Store embeddings in Vertex AI Vector Search&#10;     */&#10;    public CompletableFuture&lt;Void&gt; storeEmbeddings(String projectId, String fileId, float[] embeddings, Map&lt;String, String&gt; metadata) {&#10;        return CompletableFuture.runAsync(() -&gt; {&#10;            try {&#10;                // Create datapoint&#10;                IndexDatapoint datapoint = IndexDatapoint.newBuilder()&#10;                    .setDatapointId(fileId)&#10;                    .addAllEmbedding(floatArrayToList(embeddings))&#10;                    .putAllMetadata(convertMetadata(metadata))&#10;                    .build();&#10;                &#10;                // Upsert to index&#10;                UpsertDatapointsRequest request = UpsertDatapointsRequest.newBuilder()&#10;                    .setIndex(getIndexName(projectId))&#10;                    .addDatapoints(datapoint)&#10;                    .build();&#10;                    &#10;                indexClient.upsertDatapoints(request);&#10;                logger.debug(&quot;Stored embeddings for file: {}&quot;, fileId);&#10;                &#10;            } catch (Exception e) {&#10;                logger.error(&quot;Failed to store embeddings: {}&quot;, e.getMessage(), e);&#10;                throw new RuntimeException(&quot;Failed to store embeddings&quot;, e);&#10;            }&#10;        });&#10;    }&#10;    &#10;    /**&#10;     * Batch process files using Vertex AI Pipelines&#10;     */&#10;    public CompletableFuture&lt;BatchProcessResult&gt; batchProcessFiles(List&lt;String&gt; filePaths, String operation) {&#10;        return CompletableFuture.supplyAsync(() -&gt; {&#10;            try {&#10;                // Create batch prediction job&#10;                BatchPredictionJob.Builder jobBuilder = BatchPredictionJob.newBuilder()&#10;                    .setDisplayName(&quot;Batch &quot; + operation + &quot; - &quot; + System.currentTimeMillis())&#10;                    .setModel(getModelResourceName(operation))&#10;                    .setInputConfig(BatchPredictionJob.InputConfig.newBuilder()&#10;                        .setInstancesFormat(&quot;jsonl&quot;)&#10;                        .setGcsSource(createBatchInput(filePaths))&#10;                        .build())&#10;                    .setOutputConfig(BatchPredictionJob.OutputConfig.newBuilder()&#10;                        .setPredictionsFormat(&quot;jsonl&quot;)&#10;                        .setGcsDestination(createBatchOutput())&#10;                        .build());&#10;                &#10;                // Submit job&#10;                CreateBatchPredictionJobRequest request = CreateBatchPredictionJobRequest.newBuilder()&#10;                    .setParent(LocationName.of(projectId, location).toString())&#10;                    .setBatchPredictionJob(jobBuilder.build())&#10;                    .build();&#10;                    &#10;                predictionClient.createBatchPredictionJob(request);&#10;                &#10;                return new BatchProcessResult(&#10;                    job.getName(),&#10;                    filePaths.size(),&#10;                    operation,&#10;                    &quot;SUBMITTED&quot;&#10;                );&#10;                &#10;            } catch (Exception e) {&#10;                logger.error(&quot;Batch processing failed: {}&quot;, e.getMessage(), e);&#10;                throw new RuntimeException(&quot;Batch processing failed&quot;, e);&#10;            }&#10;        });&#10;    }&#10;    &#10;    // Helper methods&#10;    &#10;    private String buildOptimizationPrompt(String taskType, OptimizationProfile profile) {&#10;        return String.format(&#10;            &quot;Optimize this code for %s. Remove boilerplate, preserve functionality. Profile: %s&quot;,&#10;            taskType != null ? taskType : &quot;general&quot;,&#10;            profile != null ? profile.name() : &quot;BALANCED&quot;&#10;        );&#10;    }&#10;    &#10;    private double calculateOptimizationConfidence(String original, String optimized) {&#10;        // Simple confidence calculation based on preservation of key elements&#10;        // In production, use more sophisticated AST-based analysis&#10;        int originalLines = original.split(&quot;\n&quot;).length;&#10;        int optimizedLines = optimized.split(&quot;\n&quot;).length;&#10;        double reductionRatio = (double) optimizedLines / originalLines;&#10;        &#10;        // Too much reduction might indicate loss of important code&#10;        if (reductionRatio &lt; 0.3) return 0.7;&#10;        if (reductionRatio &gt; 0.9) return 0.6;&#10;        return 0.9;&#10;    }&#10;    &#10;    private int estimateTokens(String text) {&#10;        // Rough estimation: 1 token \u2248 4 characters&#10;        return text.length() / 4;&#10;    }&#10;    &#10;    private List&lt;Float&gt; floatArrayToList(float[] array) {&#10;        List&lt;Float&gt; list = new ArrayList&lt;&gt;(array.length);&#10;        for (float f : array) {&#10;            list.add(f);&#10;        }&#10;        return list;&#10;    }&#10;    &#10;    private String readFileContent(String filePath) {&#10;        // Implement file reading&#10;        return &quot;&quot;;&#10;    }&#10;    &#10;    private String getIndexEndpoint(String projectId) {&#10;        return String.format(&quot;projects/%s/locations/%s/indexEndpoints/code-search&quot;, projectId, location);&#10;    }&#10;    &#10;    private String getDeployedIndexId(String projectId) {&#10;        return &quot;deployed-code-index&quot;;&#10;    }&#10;    &#10;    private String getIndexName(String projectId) {&#10;        return IndexName.of(projectId, location, &quot;code-embeddings-index&quot;).toString();&#10;    }&#10;    &#10;    private String getModelResourceName(String operation) {&#10;        // Map operation to model endpoint&#10;        return String.format(&quot;projects/%s/locations/%s/models/gemini-flash&quot;, projectId, location);&#10;    }&#10;    &#10;    private Map&lt;String, Value&gt; convertMetadata(Map&lt;String, String&gt; metadata) {&#10;        Map&lt;String, Value&gt; result = new HashMap&lt;&gt;();&#10;        metadata.forEach((k, v) -&gt; {&#10;            Builder builder = com.google.protobuf.Value.newBuilder();&#10;            result.put(k, builder.setStringValue(v).build());&#10;        });&#10;        return result;&#10;    }&#10;    &#10;    private Map&lt;String, String&gt; extractMetadata(IndexDatapoint datapoint) {&#10;        Map&lt;String, String&gt; result = new HashMap&lt;&gt;();&#10;        datapoint.getMetadataMap().forEach((k, v) -&gt; result.put(k, v.getStringValue()));&#10;        return result;&#10;    }&#10;    &#10;    private GcsSource createBatchInput(List&lt;String&gt; filePaths) {&#10;        // Create GCS input for batch processing&#10;        return GcsSource.newBuilder()&#10;            .addUris(&quot;gs://context-engine-batch/input/&quot; + System.currentTimeMillis() + &quot;.jsonl&quot;)&#10;            .build();&#10;    }&#10;    &#10;    private GcsDestination createBatchOutput() {&#10;        // Create GCS output for batch processing&#10;        return GcsDestination.newBuilder()&#10;            .setOutputUriPrefix(&quot;gs://context-engine-batch/output/&quot; + System.currentTimeMillis())&#10;            .build();&#10;    }&#10;}" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/context-engine-mcp/src/main/java/com/contextengine/mcp/llm/LLMRouter.java">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/context-engine-mcp/src/main/java/com/contextengine/mcp/llm/LLMRouter.java" />
              <option name="originalContent" value="package com.contextengine.mcp.llm;&#10;&#10;import org.slf4j.Logger;&#10;import org.slf4j.LoggerFactory;&#10;import org.springframework.beans.factory.annotation.Autowired;&#10;import org.springframework.beans.factory.annotation.Value;&#10;import org.springframework.stereotype.Component;&#10;&#10;import java.util.*;&#10;import java.util.concurrent.CompletableFuture;&#10;import java.util.concurrent.ConcurrentHashMap;&#10;import java.util.stream.Collectors;&#10;&#10;/**&#10; * Dynamic LLM routing with automatic fallback and cost optimization&#10; * Routes requests to appropriate models based on operation type and availability&#10; */&#10;@Component&#10;public class LLMRouter {&#10;    private static final Logger logger = LoggerFactory.getLogger(LLMRouter.class);&#10;    &#10;    private final Map&lt;String, LLMProvider&gt; providers = new ConcurrentHashMap&lt;&gt;();&#10;    private final Map&lt;String, String&gt; operationDefaults = new ConcurrentHashMap&lt;&gt;();&#10;    private final Map&lt;String, List&lt;String&gt;&gt; fallbackChains = new ConcurrentHashMap&lt;&gt;();&#10;    private final ModelRegistry modelRegistry;&#10;    &#10;    @Value(&quot;${llm.router.max.cost.per.operation:1.0}&quot;)&#10;    private double maxCostPerOperation;&#10;    &#10;    @Value(&quot;${llm.router.prefer.local:true}&quot;)&#10;    private boolean preferLocal;&#10;    &#10;    @Autowired&#10;    public LLMRouter(ModelRegistry modelRegistry) {&#10;        this.modelRegistry = modelRegistry;&#10;        initializeDefaults();&#10;    }&#10;    &#10;    /**&#10;     * Initialize default model assignments per operation&#10;     */&#10;    private void initializeDefaults() {&#10;        // Default model assignments&#10;        operationDefaults.put(&quot;search&quot;, &quot;gemini-flash&quot;);&#10;        operationDefaults.put(&quot;optimize&quot;, &quot;gemini-flash&quot;);&#10;        operationDefaults.put(&quot;index&quot;, &quot;gemini-flash&quot;);&#10;        operationDefaults.put(&quot;embed&quot;, &quot;gemini-flash&quot;);&#10;        operationDefaults.put(&quot;analyze&quot;, &quot;gemini-pro&quot;);&#10;        operationDefaults.put(&quot;generate&quot;, &quot;claude-sonnet&quot;);&#10;        operationDefaults.put(&quot;security&quot;, &quot;claude-opus&quot;);&#10;        operationDefaults.put(&quot;local-complete&quot;, &quot;gemini-nano&quot;);&#10;        &#10;        // Fallback chains&#10;        fallbackChains.put(&quot;gemini-flash&quot;, List.of(&quot;gemini-pro&quot;, &quot;claude-haiku&quot;, &quot;gemini-nano&quot;));&#10;        fallbackChains.put(&quot;gemini-pro&quot;, List.of(&quot;claude-sonnet&quot;, &quot;gemini-flash&quot;));&#10;        fallbackChains.put(&quot;claude-opus&quot;, List.of(&quot;claude-sonnet&quot;, &quot;gemini-pro&quot;));&#10;        fallbackChains.put(&quot;claude-sonnet&quot;, List.of(&quot;claude-haiku&quot;, &quot;gemini-pro&quot;));&#10;        fallbackChains.put(&quot;gemini-nano&quot;, List.of(&quot;gemini-flash&quot;));&#10;    }&#10;    &#10;    /**&#10;     * Register a provider&#10;     */&#10;    public void registerProvider(LLMProvider provider) {&#10;        providers.put(provider.getId(), provider);&#10;        logger.info(&quot;Registered LLM provider: {} ({})&quot;, provider.getId(), provider.getName());&#10;    }&#10;    &#10;    /**&#10;     * Route a request to the appropriate model&#10;     */&#10;    public CompletableFuture&lt;LLMResponse&gt; route(LLMRequest request) {&#10;        String operation = request.getOperation();&#10;        String preferredModel = request.getPreferredModel();&#10;        &#10;        // Determine which model to use&#10;        String modelId = determineModel(operation, preferredModel, request);&#10;        &#10;        // Get provider with fallback&#10;        LLMProvider provider = getProviderWithFallback(modelId, request);&#10;        &#10;        if (provider == null) {&#10;            return CompletableFuture.failedFuture(&#10;                new NoAvailableProviderException(&quot;No available provider for operation: &quot; + operation)&#10;            );&#10;        }&#10;        &#10;        // Execute request&#10;        return executeRequest(provider, request);&#10;    }&#10;    &#10;    /**&#10;     * Determine which model to use based on various factors&#10;     */&#10;    private String determineModel(String operation, String preferredModel, LLMRequest request) {&#10;        // 1. Use preferred model if specified and available&#10;        if (preferredModel != null &amp;&amp; providers.containsKey(preferredModel)) {&#10;            LLMProvider provider = providers.get(preferredModel);&#10;            if (provider.isAvailable() &amp;&amp; meetsRequirements(provider, request)) {&#10;                return preferredModel;&#10;            }&#10;        }&#10;        &#10;        // 2. Check for local preference&#10;        if (preferLocal &amp;&amp; request.getRequiredLatencyMs() &lt; 50) {&#10;            LLMProvider nano = providers.get(&quot;gemini-nano&quot;);&#10;            if (nano != null &amp;&amp; nano.isAvailable() &amp;&amp; meetsRequirements(nano, request)) {&#10;                return &quot;gemini-nano&quot;;&#10;            }&#10;        }&#10;        &#10;        // 3. Use operation default&#10;        return operationDefaults.getOrDefault(operation, &quot;gemini-flash&quot;);&#10;    }&#10;    &#10;    /**&#10;     * Get provider with automatic fallback&#10;     */&#10;    private LLMProvider getProviderWithFallback(String modelId, LLMRequest request) {&#10;        // Try primary model&#10;        LLMProvider provider = providers.get(modelId);&#10;        if (provider != null &amp;&amp; provider.isAvailable() &amp;&amp; meetsRequirements(provider, request)) {&#10;            return provider;&#10;        }&#10;        &#10;        // Try fallback chain&#10;        List&lt;String&gt; fallbacks = fallbackChains.getOrDefault(modelId, List.of());&#10;        for (String fallbackId : fallbacks) {&#10;            LLMProvider fallback = providers.get(fallbackId);&#10;            if (fallback != null &amp;&amp; fallback.isAvailable() &amp;&amp; meetsRequirements(fallback, request)) {&#10;                logger.info(&quot;Falling back from {} to {}&quot;, modelId, fallbackId);&#10;                return fallback;&#10;            }&#10;        }&#10;        &#10;        // Last resort: find any available provider that meets requirements&#10;        return providers.values().stream()&#10;            .filter(p -&gt; p.isAvailable() &amp;&amp; meetsRequirements(p, request))&#10;            .findFirst()&#10;            .orElse(null);&#10;    }&#10;    &#10;    /**&#10;     * Check if provider meets request requirements&#10;     */&#10;    private boolean meetsRequirements(LLMProvider provider, LLMRequest request) {&#10;        ModelCapabilities caps = provider.getCapabilities();&#10;        &#10;        // Check capabilities&#10;        Set&lt;ModelCapability&gt; required = request.getRequiredCapabilities();&#10;        if (required != null &amp;&amp; !required.isEmpty()) {&#10;            for (ModelCapability cap : required) {&#10;                if (!caps.getCapabilities().contains(cap)) {&#10;                    return false;&#10;                }&#10;            }&#10;        }&#10;        &#10;        // Check token limit&#10;        if (request.getEstimatedTokens() &gt; caps.getMaxTokens()) {&#10;            return false;&#10;        }&#10;        &#10;        // Check cost constraint&#10;        TokenCount tokens = new TokenCount(request.getEstimatedTokens());&#10;        Cost cost = provider.estimateCost(tokens);&#10;        if (cost.getCost() &gt; maxCostPerOperation) {&#10;            return false;&#10;        }&#10;        &#10;        // Check latency requirement&#10;        if (request.getRequiredLatencyMs() &gt; 0 &amp;&amp; caps.getAverageLatencyMs() &gt; request.getRequiredLatencyMs()) {&#10;            return false;&#10;        }&#10;        &#10;        return true;&#10;    }&#10;    &#10;    /**&#10;     * Execute request with the selected provider&#10;     */&#10;    private CompletableFuture&lt;LLMResponse&gt; executeRequest(LLMProvider provider, LLMRequest request) {&#10;        logger.debug(&quot;Routing {} request to {}&quot;, request.getOperation(), provider.getId());&#10;        &#10;        if (request.getCompletionRequest() != null) {&#10;            return provider.complete(request.getCompletionRequest())&#10;                .thenApply(response -&gt; new LLMResponse(provider.getId(), response, null));&#10;        } else if (request.getEmbeddingRequest() != null) {&#10;            return provider.embed(request.getEmbeddingRequest())&#10;                .thenApply(response -&gt; new LLMResponse(provider.getId(), null, response));&#10;        } else {&#10;            return CompletableFuture.failedFuture(&#10;                new IllegalArgumentException(&quot;Unknown request type: &quot; + request.getOperation())&#10;            );&#10;        }&#10;    }&#10;    &#10;    /**&#10;     * Set model for a specific operation&#10;     */&#10;    public void setModelForOperation(String operation, String modelId) {&#10;        if (!providers.containsKey(modelId)) {&#10;            throw new IllegalArgumentException(&quot;Unknown model: &quot; + modelId);&#10;        }&#10;        operationDefaults.put(operation, modelId);&#10;        logger.info(&quot;Set model for operation '{}' to '{}'&quot;, operation, modelId);&#10;    }&#10;    &#10;    /**&#10;     * Get current model assignments&#10;     */&#10;    public Map&lt;String, String&gt; getModelAssignments() {&#10;        return new HashMap&lt;&gt;(operationDefaults);&#10;    }&#10;    &#10;    /**&#10;     * Get available providers&#10;     */&#10;    public List&lt;ProviderInfo&gt; getAvailableProviders() {&#10;        return providers.values().stream()&#10;            .map(p -&gt; new ProviderInfo(&#10;                p.getId(),&#10;                p.getName(),&#10;                p.isAvailable(),&#10;                p.getCapabilities(),&#10;                p.getUsageStats()&#10;            ))&#10;            .collect(Collectors.toList());&#10;    }&#10;    &#10;    /**&#10;     * Find best provider for given capabilities&#10;     */&#10;    public LLMProvider findBestProvider(Set&lt;ModelCapability&gt; requiredCapabilities, int maxLatencyMs, double maxCost) {&#10;        return providers.values().stream()&#10;            .filter(p -&gt; p.isAvailable())&#10;            .filter(p -&gt; {&#10;                ModelCapabilities caps = p.getCapabilities();&#10;                // Check all required capabilities&#10;                for (ModelCapability req : requiredCapabilities) {&#10;                    if (!caps.getCapabilities().contains(req)) return false;&#10;                }&#10;                // Check latency&#10;                if (maxLatencyMs &gt; 0 &amp;&amp; caps.getAverageLatencyMs() &gt; maxLatencyMs) return false;&#10;                // Check cost&#10;                if (maxCost &gt; 0 &amp;&amp; caps.getCostPerMillionTokens() &gt; maxCost * 1000000) return false;&#10;                return true;&#10;            })&#10;            .min(Comparator.comparing(p -&gt; {&#10;                // Prefer: local &gt; low cost &gt; low latency&#10;                ModelCapabilities caps = p.getCapabilities();&#10;                double score = 0;&#10;                if (caps.isLocal()) score -= 1000;  // Strong preference for local&#10;                score += caps.getCostPerMillionTokens();&#10;                score += caps.getAverageLatencyMs() / 1000.0;&#10;                return score;&#10;            }))&#10;            .orElse(null);&#10;    }&#10;}" />
              <option name="updatedContent" value="package com.contextengine.mcp.llm;&#10;&#10;import org.slf4j.Logger;&#10;import org.slf4j.LoggerFactory;&#10;import org.springframework.beans.factory.annotation.Autowired;&#10;import org.springframework.beans.factory.annotation.Value;&#10;import org.springframework.stereotype.Component;&#10;&#10;import java.util.*;&#10;import java.util.concurrent.CompletableFuture;&#10;import java.util.concurrent.ConcurrentHashMap;&#10;import java.util.stream.Collectors;&#10;&#10;/**&#10; * Dynamic LLM routing with automatic fallback and cost optimization&#10; * Routes requests to appropriate models based on operation type and availability&#10; */&#10;@Component&#10;public class LLMRouter {&#10;    private static final Logger logger = LoggerFactory.getLogger(LLMRouter.class);&#10;    &#10;    private final Map&lt;String, LLMProvider&gt; providers = new ConcurrentHashMap&lt;&gt;();&#10;    private final Map&lt;String, String&gt; operationDefaults = new ConcurrentHashMap&lt;&gt;();&#10;    private final Map&lt;String, List&lt;String&gt;&gt; fallbackChains = new ConcurrentHashMap&lt;&gt;();&#10;    private final ModelRegistry modelRegistry;&#10;    &#10;    @Value(&quot;${llm.router.max.cost.per.operation:1.0}&quot;)&#10;    private double maxCostPerOperation;&#10;    &#10;    @Value(&quot;${llm.router.prefer.local:true}&quot;)&#10;    private boolean preferLocal;&#10;    &#10;    @Autowired&#10;    public LLMRouter(ModelRegistry modelRegistry) {&#10;        this.modelRegistry = modelRegistry;&#10;        initializeDefaults();&#10;    }&#10;    &#10;    /**&#10;     * Initialize default model assignments per operation&#10;     */&#10;    private void initializeDefaults() {&#10;        // Default model assignments&#10;        operationDefaults.put(&quot;search&quot;, &quot;gemini-flash&quot;);&#10;        operationDefaults.put(&quot;optimize&quot;, &quot;gemini-flash&quot;);&#10;        operationDefaults.put(&quot;index&quot;, &quot;gemini-flash&quot;);&#10;        operationDefaults.put(&quot;embed&quot;, &quot;gemini-flash&quot;);&#10;        operationDefaults.put(&quot;analyze&quot;, &quot;gemini-pro&quot;);&#10;        operationDefaults.put(&quot;generate&quot;, &quot;claude-sonnet&quot;);&#10;        operationDefaults.put(&quot;security&quot;, &quot;claude-opus&quot;);&#10;        operationDefaults.put(&quot;local-complete&quot;, &quot;gemini-nano&quot;);&#10;        &#10;        // Fallback chains&#10;        fallbackChains.put(&quot;gemini-flash&quot;, List.of(&quot;gemini-pro&quot;, &quot;claude-haiku&quot;, &quot;gemini-nano&quot;));&#10;        fallbackChains.put(&quot;gemini-pro&quot;, List.of(&quot;claude-sonnet&quot;, &quot;gemini-flash&quot;));&#10;        fallbackChains.put(&quot;claude-opus&quot;, List.of(&quot;claude-sonnet&quot;, &quot;gemini-pro&quot;));&#10;        fallbackChains.put(&quot;claude-sonnet&quot;, List.of(&quot;claude-haiku&quot;, &quot;gemini-pro&quot;));&#10;        fallbackChains.put(&quot;gemini-nano&quot;, List.of(&quot;gemini-flash&quot;));&#10;    }&#10;    &#10;    /**&#10;     * Register a provider&#10;     */&#10;    public void registerProvider(LLMProvider provider) {&#10;        providers.put(provider.getId(), provider);&#10;        logger.info(&quot;Registered LLM provider: {} ({})&quot;, provider.getId(), provider.getName());&#10;    }&#10;    &#10;    /**&#10;     * Route a request to the appropriate model, with logging and error handling.&#10;     * @param request LLMRequest to route&#10;     * @return CompletableFuture with LLMResponse or failed future if no provider&#10;     */&#10;    public CompletableFuture&lt;LLMResponse&gt; route(LLMRequest request) {&#10;        String operation = request.getOperation();&#10;        String preferredModel = request.getPreferredModel();&#10;        String modelId = determineModel(operation, preferredModel, request);&#10;        LLMProvider provider = getProviderWithFallback(modelId, request);&#10;        if (provider == null) {&#10;            logger.error(&quot;No available provider for operation: {} (preferred: {})&quot;, operation, preferredModel);&#10;            return CompletableFuture.failedFuture(&#10;                new NoAvailableProviderException(&quot;No available provider for operation: &quot; + operation)&#10;            );&#10;        }&#10;        logger.info(&quot;Routing operation '{}' to provider '{}' (model: {})&quot;, operation, provider.getName(), modelId);&#10;        try {&#10;            return executeRequest(provider, request)&#10;                .exceptionally(ex -&gt; {&#10;                    logger.error(&quot;Request execution failed for provider {}: {}&quot;, provider.getName(), ex.getMessage(), ex);&#10;                    throw new RuntimeException(&quot;LLM request failed&quot;, ex);&#10;                });&#10;        } catch (Exception e) {&#10;            logger.error(&quot;Unexpected error routing request: {}&quot;, e.getMessage(), e);&#10;            return CompletableFuture.failedFuture(e);&#10;        }&#10;    }&#10;    &#10;    /**&#10;     * Determine which model to use based on various factors&#10;     */&#10;    private String determineModel(String operation, String preferredModel, LLMRequest request) {&#10;        // 1. Use preferred model if specified and available&#10;        if (preferredModel != null &amp;&amp; providers.containsKey(preferredModel)) {&#10;            LLMProvider provider = providers.get(preferredModel);&#10;            if (provider.isAvailable() &amp;&amp; meetsRequirements(provider, request)) {&#10;                return preferredModel;&#10;            }&#10;        }&#10;        &#10;        // 2. Check for local preference&#10;        if (preferLocal &amp;&amp; request.getRequiredLatencyMs() &lt; 50) {&#10;            LLMProvider nano = providers.get(&quot;gemini-nano&quot;);&#10;            if (nano != null &amp;&amp; nano.isAvailable() &amp;&amp; meetsRequirements(nano, request)) {&#10;                return &quot;gemini-nano&quot;;&#10;            }&#10;        }&#10;        &#10;        // 3. Use operation default&#10;        return operationDefaults.getOrDefault(operation, &quot;gemini-flash&quot;);&#10;    }&#10;    &#10;    /**&#10;     * Get provider with automatic fallback&#10;     */&#10;    private LLMProvider getProviderWithFallback(String modelId, LLMRequest request) {&#10;        // Try primary model&#10;        LLMProvider provider = providers.get(modelId);&#10;        if (provider != null &amp;&amp; provider.isAvailable() &amp;&amp; meetsRequirements(provider, request)) {&#10;            return provider;&#10;        }&#10;        &#10;        // Try fallback chain&#10;        List&lt;String&gt; fallbacks = fallbackChains.getOrDefault(modelId, List.of());&#10;        for (String fallbackId : fallbacks) {&#10;            LLMProvider fallback = providers.get(fallbackId);&#10;            if (fallback != null &amp;&amp; fallback.isAvailable() &amp;&amp; meetsRequirements(fallback, request)) {&#10;                logger.info(&quot;Falling back from {} to {}&quot;, modelId, fallbackId);&#10;                return fallback;&#10;            }&#10;        }&#10;        &#10;        // Last resort: find any available provider that meets requirements&#10;        return providers.values().stream()&#10;            .filter(p -&gt; p.isAvailable() &amp;&amp; meetsRequirements(p, request))&#10;            .findFirst()&#10;            .orElse(null);&#10;    }&#10;    &#10;    /**&#10;     * Check if provider meets request requirements&#10;     */&#10;    private boolean meetsRequirements(LLMProvider provider, LLMRequest request) {&#10;        ModelCapabilities caps = provider.getCapabilities();&#10;        &#10;        // Check capabilities&#10;        Set&lt;ModelCapability&gt; required = request.getRequiredCapabilities();&#10;        if (required != null &amp;&amp; !required.isEmpty()) {&#10;            for (ModelCapability cap : required) {&#10;                if (!caps.getCapabilities().contains(cap)) {&#10;                    return false;&#10;                }&#10;            }&#10;        }&#10;        &#10;        // Check token limit&#10;        if (request.getEstimatedTokens() &gt; caps.getMaxTokens()) {&#10;            return false;&#10;        }&#10;        &#10;        // Check cost constraint&#10;        TokenCount tokens = new TokenCount(request.getEstimatedTokens());&#10;        Cost cost = provider.estimateCost(tokens);&#10;        if (cost.getCost() &gt; maxCostPerOperation) {&#10;            return false;&#10;        }&#10;        &#10;        // Check latency requirement&#10;        if (request.getRequiredLatencyMs() &gt; 0 &amp;&amp; caps.getAverageLatencyMs() &gt; request.getRequiredLatencyMs()) {&#10;            return false;&#10;        }&#10;        &#10;        return true;&#10;    }&#10;    &#10;    /**&#10;     * Execute request with the selected provider&#10;     */&#10;    private CompletableFuture&lt;LLMResponse&gt; executeRequest(LLMProvider provider, LLMRequest request) {&#10;        logger.debug(&quot;Routing {} request to {}&quot;, request.getOperation(), provider.getId());&#10;        &#10;        if (request.getCompletionRequest() != null) {&#10;            return provider.complete(request.getCompletionRequest())&#10;                .thenApply(response -&gt; new LLMResponse(provider.getId(), response, null));&#10;        } else if (request.getEmbeddingRequest() != null) {&#10;            return provider.embed(request.getEmbeddingRequest())&#10;                .thenApply(response -&gt; new LLMResponse(provider.getId(), null, response));&#10;        } else {&#10;            return CompletableFuture.failedFuture(&#10;                new IllegalArgumentException(&quot;Unknown request type: &quot; + request.getOperation())&#10;            );&#10;        }&#10;    }&#10;    &#10;    /**&#10;     * Set model for a specific operation&#10;     */&#10;    public void setModelForOperation(String operation, String modelId) {&#10;        if (!providers.containsKey(modelId)) {&#10;            throw new IllegalArgumentException(&quot;Unknown model: &quot; + modelId);&#10;        }&#10;        operationDefaults.put(operation, modelId);&#10;        logger.info(&quot;Set model for operation '{}' to '{}'&quot;, operation, modelId);&#10;    }&#10;    &#10;    /**&#10;     * Get current model assignments&#10;     */&#10;    public Map&lt;String, String&gt; getModelAssignments() {&#10;        return new HashMap&lt;&gt;(operationDefaults);&#10;    }&#10;    &#10;    /**&#10;     * Get available providers&#10;     */&#10;    public List&lt;ProviderInfo&gt; getAvailableProviders() {&#10;        return providers.values().stream()&#10;            .map(p -&gt; new ProviderInfo(&#10;                p.getId(),&#10;                p.getName(),&#10;                p.isAvailable(),&#10;                p.getCapabilities(),&#10;                p.getUsageStats()&#10;            ))&#10;            .collect(Collectors.toList());&#10;    }&#10;    &#10;    /**&#10;     * Find best provider for given capabilities&#10;     */&#10;    public LLMProvider findBestProvider(Set&lt;ModelCapability&gt; requiredCapabilities, int maxLatencyMs, double maxCost) {&#10;        return providers.values().stream()&#10;            .filter(p -&gt; p.isAvailable())&#10;            .filter(p -&gt; {&#10;                ModelCapabilities caps = p.getCapabilities();&#10;                // Check all required capabilities&#10;                for (ModelCapability req : requiredCapabilities) {&#10;                    if (!caps.getCapabilities().contains(req)) return false;&#10;                }&#10;                // Check latency&#10;                if (maxLatencyMs &gt; 0 &amp;&amp; caps.getAverageLatencyMs() &gt; maxLatencyMs) return false;&#10;                // Check cost&#10;                if (maxCost &gt; 0 &amp;&amp; caps.getCostPerMillionTokens() &gt; maxCost * 1000000) return false;&#10;                return true;&#10;            })&#10;            .min(Comparator.comparing(p -&gt; {&#10;                // Prefer: local &gt; low cost &gt; low latency&#10;                ModelCapabilities caps = p.getCapabilities();&#10;                double score = 0;&#10;                if (caps.isLocal()) score -= 1000;  // Strong preference for local&#10;                score += caps.getCostPerMillionTokens();&#10;                score += caps.getAverageLatencyMs() / 1000.0;&#10;                return score;&#10;            }))&#10;            .orElse(null);&#10;    }&#10;}&#10;" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/context-engine-mcp/src/main/java/com/contextengine/mcp/sdk/MCPServer.java">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/context-engine-mcp/src/main/java/com/contextengine/mcp/sdk/MCPServer.java" />
              <option name="originalContent" value="package com.contextengine.mcp.sdk;&#10;&#10;import com.github.arteam.simplejsonrpc.server.JsonRpcServer;&#10;&#10;import java.util.ArrayList;&#10;import java.util.List;&#10;import java.util.Map;&#10;import java.util.concurrent.ConcurrentHashMap;&#10;&#10;/**&#10; * Simple MCP Server implementation using JSON-RPC&#10; */&#10;public class MCPServer {&#10;    private final String name;&#10;    private final String version;&#10;    private final List&lt;Tool&gt; tools = new ArrayList&lt;&gt;();&#10;    private final JsonRpcServer jsonRpcServer = new JsonRpcServer();&#10;    &#10;    public MCPServer(String name, String version) {&#10;        this.name = name;&#10;        this.version = version;&#10;        // JSON-RPC server already initialized&#10;    }&#10;    &#10;    public void registerTool(Tool tool) {&#10;        tools.add(tool);&#10;    }&#10;    &#10;    public void connect(Transport transport) {&#10;        // Connect to transport&#10;    }&#10;    &#10;    public void serve() throws InterruptedException {&#10;        // Keep server running&#10;        Thread.currentThread().join();&#10;    }&#10;    &#10;    public void close() {&#10;        // Cleanup&#10;    }&#10;    &#10;    public static class Builder {&#10;        private String name;&#10;        private String version;&#10;        &#10;        public Builder withName(String name) {&#10;            this.name = name;&#10;            return this;&#10;        }&#10;        &#10;        public Builder withVersion(String version) {&#10;            this.version = version;&#10;            return this;&#10;        }&#10;        &#10;        public Builder withDescription(String description) {&#10;            return this;&#10;        }&#10;        &#10;        public Builder withTool(Tool tool) {&#10;            return this;&#10;        }&#10;        &#10;        public Builder withCapability(String name, boolean value) {&#10;            return this;&#10;        }&#10;        &#10;        public MCPServer build() {&#10;            return new MCPServer(name, version);&#10;        }&#10;    }&#10;    &#10;    public static Builder create(String name) {&#10;        return new Builder().withName(name);&#10;    }&#10;}&#10;&#10;" />
              <option name="updatedContent" value="package com.contextengine.mcp.sdk;&#10;&#10;import com.github.arteam.simplejsonrpc.server.JsonRpcServer;&#10;import org.slf4j.Logger;&#10;import org.slf4j.LoggerFactory;&#10;&#10;import java.util.ArrayList;&#10;import java.util.List;&#10;import java.util.Map;&#10;import java.util.concurrent.ConcurrentHashMap;&#10;&#10;/**&#10; * Simple MCP Server implementation using JSON-RPC&#10; */&#10;public class MCPServer {&#10;    private static final Logger logger = LoggerFactory.getLogger(MCPServer.class);&#10;    private final String name;&#10;    private final String version;&#10;    private final List&lt;Tool&gt; tools = new ArrayList&lt;&gt;();&#10;    private final JsonRpcServer jsonRpcServer = new JsonRpcServer();&#10;    &#10;    public MCPServer(String name, String version) {&#10;        this.name = name;&#10;        this.version = version;&#10;        // JSON-RPC server already initialized&#10;    }&#10;    &#10;    /**&#10;     * Register a tool with the MCP server.&#10;     * @param tool Tool to register&#10;     */&#10;    public void registerTool(Tool tool) {&#10;        tools.add(tool);&#10;        logger.info(&quot;Registered tool: {}&quot;, tool.getClass().getSimpleName());&#10;    }&#10;&#10;    /**&#10;     * Connects the server to the specified transport.&#10;     * @param transport Transport to connect&#10;     */&#10;    public void connect(Transport transport) {&#10;        try {&#10;            // Connect to transport (stub)&#10;            logger.info(&quot;Connecting MCPServer to transport: {}&quot;, transport.getClass().getSimpleName());&#10;        } catch (Exception e) {&#10;            logger.error(&quot;Failed to connect to transport: {}&quot;, e.getMessage(), e);&#10;            throw new RuntimeException(&quot;Transport connection failed&quot;, e);&#10;        }&#10;    }&#10;&#10;    /**&#10;     * Keeps the server running, blocking the current thread.&#10;     * @throws InterruptedException if interrupted&#10;     */&#10;    public void serve() throws InterruptedException {&#10;        logger.info(&quot;MCPServer is now serving requests.&quot;);&#10;        Thread.currentThread().join();&#10;    }&#10;&#10;    /**&#10;     * Cleans up server resources.&#10;     */&#10;    public void close() {&#10;        try {&#10;            logger.info(&quot;Closing MCPServer and cleaning up resources.&quot;);&#10;            // Cleanup logic (stub)&#10;        } catch (Exception e) {&#10;            logger.error(&quot;Error during MCPServer cleanup: {}&quot;, e.getMessage(), e);&#10;        }&#10;    }&#10;    &#10;    /**&#10;     * Builder for MCPServer.&#10;     */&#10;    public static class Builder {&#10;        private String name;&#10;        private String version;&#10;        &#10;        /**&#10;         * Sets the server name.&#10;         */&#10;        public Builder withName(String name) {&#10;            this.name = name;&#10;            return this;&#10;        }&#10;        &#10;        /**&#10;         * Sets the server version.&#10;         */&#10;        public Builder withVersion(String version) {&#10;            this.version = version;&#10;            return this;&#10;        }&#10;        &#10;        /**&#10;         * Sets the server description (currently unused).&#10;         */&#10;        public Builder withDescription(String description) {&#10;            return this;&#10;        }&#10;        &#10;        /**&#10;         * Adds a tool to the server (currently unused).&#10;         */&#10;        public Builder withTool(Tool tool) {&#10;            return this;&#10;        }&#10;        &#10;        /**&#10;         * Adds a capability to the server (currently unused).&#10;         */&#10;        public Builder withCapability(String name, boolean value) {&#10;            return this;&#10;        }&#10;        &#10;        /**&#10;         * Builds the MCPServer instance.&#10;         */&#10;        public MCPServer build() {&#10;            return new MCPServer(name, version);&#10;        }&#10;    }&#10;    &#10;    public static Builder create(String name) {&#10;        return new Builder().withName(name);&#10;    }&#10;}" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/context-engine-mcp/src/main/java/com/contextengine/mcp/tools/AnalyzeCodebaseTool.java">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/context-engine-mcp/src/main/java/com/contextengine/mcp/tools/AnalyzeCodebaseTool.java" />
              <option name="originalContent" value="package com.contextengine.mcp.tools;&#10;&#10;import com.contextengine.mcp.adk.GoogleADKService;&#10;import com.contextengine.mcp.sdk.Tool;&#10;import com.contextengine.mcp.sdk.ToolDefinition;&#10;import com.contextengine.mcp.sdk.ToolParameter;&#10;import org.springframework.stereotype.Component;&#10;&#10;import java.util.Map;&#10;import java.util.List;&#10;import java.util.concurrent.CompletableFuture;&#10;&#10;/**&#10; * MCP Tool: Analyze entire codebase structure and patterns&#10; */&#10;@Component&#10;public class AnalyzeCodebaseTool implements Tool {&#10;    &#10;    private final GoogleADKService adkService;&#10;    &#10;    public AnalyzeCodebaseTool(GoogleADKService adkService) {&#10;        this.adkService = adkService;&#10;    }&#10;    &#10;    @Override&#10;    public ToolDefinition getDefinition() {&#10;        return ToolDefinition.builder()&#10;            .name(&quot;analyze_codebase&quot;)&#10;            .description(&quot;Analyze codebase structure, patterns, and architecture&quot;)&#10;            .parameter(ToolParameter.builder()&#10;                .name(&quot;projectPath&quot;)&#10;                .type(&quot;string&quot;)&#10;                .description(&quot;Root path of the project to analyze&quot;)&#10;                .required(true)&#10;                .build())&#10;            .parameter(ToolParameter.builder()&#10;                .name(&quot;depth&quot;)&#10;                .type(&quot;string&quot;)&#10;                .description(&quot;Analysis depth (quick, standard, deep)&quot;)&#10;                .required(false)&#10;                .defaultValue(&quot;standard&quot;)&#10;                .build())&#10;            .build();&#10;    }&#10;    &#10;    @Override&#10;    public CompletableFuture&lt;Object&gt; execute(Map&lt;String, Object&gt; arguments) {&#10;        String projectPath = (String) arguments.get(&quot;projectPath&quot;);&#10;        String depth = (String) arguments.getOrDefault(&quot;depth&quot;, &quot;standard&quot;);&#10;        &#10;        // TODO: Implement actual codebase analysis using batch processing&#10;        return CompletableFuture.completedFuture(Map.of(&#10;            &quot;projectPath&quot;, projectPath,&#10;            &quot;summary&quot;, Map.of(&#10;                &quot;totalFiles&quot;, 150,&#10;                &quot;languages&quot;, List.of(&quot;Java&quot;, &quot;Python&quot;, &quot;JavaScript&quot;),&#10;                &quot;frameworks&quot;, List.of(&quot;Spring Boot&quot;, &quot;React&quot;, &quot;FastAPI&quot;),&#10;                &quot;architecture&quot;, &quot;Microservices with gRPC&quot;&#10;            ),&#10;            &quot;patterns&quot;, List.of(&#10;                &quot;Repository Pattern&quot;,&#10;                &quot;Factory Pattern&quot;,&#10;                &quot;Observer Pattern&quot;&#10;            ),&#10;            &quot;recommendations&quot;, List.of(&#10;                &quot;Consider adding more unit tests (current coverage: 65%)&quot;,&#10;                &quot;Reduce cyclomatic complexity in PaymentService&quot;,&#10;                &quot;Update deprecated dependencies&quot;&#10;            )&#10;        ));&#10;    }&#10;}" />
              <option name="updatedContent" value="package com.contextengine.mcp.tools;&#10;&#10;import com.contextengine.mcp.adk.GoogleADKService;&#10;import com.contextengine.mcp.sdk.Tool;&#10;import com.contextengine.mcp.sdk.ToolDefinition;&#10;import com.contextengine.mcp.sdk.ToolParameter;&#10;import org.springframework.stereotype.Component;&#10;&#10;import java.util.Map;&#10;import java.util.List;&#10;import java.util.concurrent.CompletableFuture;&#10;&#10;/**&#10; * MCP Tool: Analyze entire codebase structure and patterns&#10; */&#10;@Component&#10;public class AnalyzeCodebaseTool implements Tool {&#10;    &#10;    private final GoogleADKService adkService;&#10;    &#10;    public AnalyzeCodebaseTool(GoogleADKService adkService) {&#10;        this.adkService = adkService;&#10;    }&#10;    &#10;    @Override&#10;    public ToolDefinition getDefinition() {&#10;        return ToolDefinition.builder()&#10;            .name(&quot;analyze_codebase&quot;)&#10;            .description(&quot;Analyze codebase structure, patterns, and architecture&quot;)&#10;            .parameter(ToolParameter.builder()&#10;                .name(&quot;projectPath&quot;)&#10;                .type(&quot;string&quot;)&#10;                .description(&quot;Root path of the project to analyze&quot;)&#10;                .required(true)&#10;                .build())&#10;            .parameter(ToolParameter.builder()&#10;                .name(&quot;depth&quot;)&#10;                .type(&quot;string&quot;)&#10;                .description(&quot;Analysis depth (quick, standard, deep)&quot;)&#10;                .required(false)&#10;                .defaultValue(&quot;standard&quot;)&#10;                .build())&#10;            .build();&#10;    }&#10;    &#10;    /**&#10;     * Executes codebase analysis. Currently returns a static result.&#10;     * TODO: Implement actual codebase analysis using batch processing and AI models.&#10;     * @param arguments Map of arguments (projectPath, depth)&#10;     * @return CompletableFuture with analysis result&#10;     */&#10;    @Override&#10;    public CompletableFuture&lt;Object&gt; execute(Map&lt;String, Object&gt; arguments) {&#10;        String projectPath = (String) arguments.get(&quot;projectPath&quot;);&#10;        String depth = (String) arguments.getOrDefault(&quot;depth&quot;, &quot;standard&quot;);&#10;        if (projectPath == null || projectPath.isEmpty()) {&#10;            org.slf4j.LoggerFactory.getLogger(getClass()).warn(&quot;projectPath argument is missing or empty&quot;);&#10;            return CompletableFuture.failedFuture(new IllegalArgumentException(&quot;projectPath is required&quot;));&#10;        }&#10;        // TODO: Implement actual codebase analysis using batch processing&#10;        return CompletableFuture.completedFuture(Map.of(&#10;            &quot;projectPath&quot;, projectPath,&#10;            &quot;summary&quot;, Map.of(&#10;                &quot;totalFiles&quot;, 150,&#10;                &quot;languages&quot;, List.of(&quot;Java&quot;, &quot;Python&quot;, &quot;JavaScript&quot;),&#10;                &quot;frameworks&quot;, List.of(&quot;Spring Boot&quot;, &quot;React&quot;, &quot;FastAPI&quot;),&#10;                &quot;architecture&quot;, &quot;Microservices with gRPC&quot;&#10;            ),&#10;            &quot;patterns&quot;, List.of(&#10;                &quot;Repository Pattern&quot;,&#10;                &quot;Factory Pattern&quot;,&#10;                &quot;Observer Pattern&quot;&#10;            ),&#10;            &quot;recommendations&quot;, List.of(&#10;                &quot;Consider adding more unit tests (current coverage: 65%)&quot;,&#10;                &quot;Reduce cyclomatic complexity in PaymentService&quot;,&#10;                &quot;Update deprecated dependencies&quot;&#10;            )&#10;        ));&#10;    }&#10;}" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/context-engine-mcp/src/main/java/com/contextengine/mcp/tools/GetProjectMemoryTool.java">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/context-engine-mcp/src/main/java/com/contextengine/mcp/tools/GetProjectMemoryTool.java" />
              <option name="originalContent" value="package com.contextengine.mcp.tools;&#10;&#10;import com.contextengine.mcp.sdk.Tool;&#10;import com.contextengine.mcp.sdk.ToolDefinition;&#10;import com.contextengine.mcp.sdk.ToolParameter;&#10;import org.springframework.stereotype.Component;&#10;&#10;import java.util.Map;&#10;import java.util.List;&#10;import java.util.concurrent.CompletableFuture;&#10;&#10;/**&#10; * MCP Tool: Retrieve project memory and insights&#10; */&#10;@Component&#10;public class GetProjectMemoryTool implements Tool {&#10;    &#10;    @Override&#10;    public ToolDefinition getDefinition() {&#10;        return ToolDefinition.builder()&#10;            .name(&quot;get_project_memory&quot;)&#10;            .description(&quot;Retrieve stored insights and patterns for a project&quot;)&#10;            .parameter(ToolParameter.builder()&#10;                .name(&quot;projectId&quot;)&#10;                .type(&quot;string&quot;)&#10;                .description(&quot;The project ID&quot;)&#10;                .required(true)&#10;                .build())&#10;            .parameter(ToolParameter.builder()&#10;                .name(&quot;filter&quot;)&#10;                .type(&quot;string&quot;)&#10;                .description(&quot;Filter type (patterns, decisions, bugs, conventions)&quot;)&#10;                .required(false)&#10;                .build())&#10;            .build();&#10;    }&#10;    &#10;    @Override&#10;    public CompletableFuture&lt;Object&gt; execute(Map&lt;String, Object&gt; arguments) {&#10;        String projectId = (String) arguments.get(&quot;projectId&quot;);&#10;        String filter = (String) arguments.get(&quot;filter&quot;);&#10;        &#10;        // TODO: Implement actual memory retrieval from database&#10;        return CompletableFuture.completedFuture(Map.of(&#10;            &quot;projectId&quot;, projectId,&#10;            &quot;insights&quot;, List.of(&#10;                Map.of(&quot;type&quot;, &quot;pattern&quot;, &quot;content&quot;, &quot;Uses Repository pattern for data access&quot;),&#10;                Map.of(&quot;type&quot;, &quot;convention&quot;, &quot;content&quot;, &quot;camelCase for method names&quot;),&#10;                Map.of(&quot;type&quot;, &quot;decision&quot;, &quot;content&quot;, &quot;Chose gRPC for inter-service communication&quot;)&#10;            ),&#10;            &quot;lastUpdated&quot;, System.currentTimeMillis()&#10;        ));&#10;    }&#10;}" />
              <option name="updatedContent" value="package com.contextengine.mcp.tools;&#10;&#10;import com.contextengine.mcp.sdk.Tool;&#10;import com.contextengine.mcp.sdk.ToolDefinition;&#10;import com.contextengine.mcp.sdk.ToolParameter;&#10;import org.springframework.stereotype.Component;&#10;import org.slf4j.Logger;&#10;import org.slf4j.LoggerFactory;&#10;&#10;import java.util.Map;&#10;import java.util.List;&#10;import java.util.concurrent.CompletableFuture;&#10;&#10;/**&#10; * MCP Tool: Retrieve project memory and insights&#10; */&#10;@Component&#10;public class GetProjectMemoryTool implements Tool {&#10;    private static final Logger logger = LoggerFactory.getLogger(GetProjectMemoryTool.class);&#10;    &#10;    @Override&#10;    public ToolDefinition getDefinition() {&#10;        return ToolDefinition.builder()&#10;            .name(&quot;get_project_memory&quot;)&#10;            .description(&quot;Retrieve stored insights and patterns for a project&quot;)&#10;            .parameter(ToolParameter.builder()&#10;                .name(&quot;projectId&quot;)&#10;                .type(&quot;string&quot;)&#10;                .description(&quot;The project ID&quot;)&#10;                .required(true)&#10;                .build())&#10;            .parameter(ToolParameter.builder()&#10;                .name(&quot;filter&quot;)&#10;                .type(&quot;string&quot;)&#10;                .description(&quot;Filter type (patterns, decisions, bugs, conventions)&quot;)&#10;                .required(false)&#10;                .build())&#10;            .build();&#10;    }&#10;    &#10;    /**&#10;     * Retrieves stored insights and patterns for a project.&#10;     * TODO: Implement actual memory retrieval from database.&#10;     * @param arguments Map of arguments (projectId, filter)&#10;     * @return CompletableFuture with project memory insights&#10;     */&#10;    @Override&#10;    public CompletableFuture&lt;Object&gt; execute(Map&lt;String, Object&gt; arguments) {&#10;        String projectId = (String) arguments.get(&quot;projectId&quot;);&#10;        String filter = (String) arguments.get(&quot;filter&quot;);&#10;        if (projectId == null || projectId.isEmpty()) {&#10;            logger.warn(&quot;Missing required argument: projectId&quot;);&#10;            return CompletableFuture.failedFuture(new IllegalArgumentException(&quot;projectId is required&quot;));&#10;        }&#10;        logger.info(&quot;Retrieving project memory for projectId='{}', filter='{}'&quot;, projectId, filter);&#10;        // TODO: Implement actual memory retrieval from database&#10;        return CompletableFuture.completedFuture(Map.of(&#10;            &quot;projectId&quot;, projectId,&#10;            &quot;insights&quot;, List.of(&#10;                Map.of(&quot;type&quot;, &quot;pattern&quot;, &quot;content&quot;, &quot;Uses Repository pattern for data access&quot;),&#10;                Map.of(&quot;type&quot;, &quot;convention&quot;, &quot;content&quot;, &quot;camelCase for method names&quot;),&#10;                Map.of(&quot;type&quot;, &quot;decision&quot;, &quot;content&quot;, &quot;Chose gRPC for inter-service communication&quot;)&#10;            ),&#10;            &quot;filter&quot;, filter&#10;        ));&#10;    }&#10;}" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/context-engine-mcp/src/main/java/com/contextengine/mcp/tools/OptimizeContextTool.java">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/context-engine-mcp/src/main/java/com/contextengine/mcp/tools/OptimizeContextTool.java" />
              <option name="originalContent" value="package com.contextengine.mcp.tools;&#10;&#10;import com.contextengine.mcp.adk.GoogleADKService;&#10;import com.contextengine.mcp.adk.OptimizationProfile;&#10;import com.contextengine.mcp.sdk.Tool;&#10;import com.contextengine.mcp.sdk.ToolDefinition;&#10;import com.contextengine.mcp.sdk.ToolParameter;&#10;import org.springframework.stereotype.Component;&#10;&#10;import java.util.Map;&#10;import java.util.concurrent.CompletableFuture;&#10;&#10;/**&#10; * MCP Tool: Optimize file context using Google ADK models&#10; */&#10;@Component&#10;public class OptimizeContextTool implements Tool {&#10;    &#10;    private final GoogleADKService adkService;&#10;    &#10;    public OptimizeContextTool(GoogleADKService adkService) {&#10;        this.adkService = adkService;&#10;    }&#10;    &#10;    @Override&#10;    public ToolDefinition getDefinition() {&#10;        return ToolDefinition.builder()&#10;            .name(&quot;optimize_context&quot;)&#10;            .description(&quot;Optimize file context by removing boilerplate and keeping relevant code&quot;)&#10;            .parameter(ToolParameter.builder()&#10;                .name(&quot;filePath&quot;)&#10;                .type(&quot;string&quot;)&#10;                .description(&quot;Path to the file to optimize&quot;)&#10;                .required(true)&#10;                .build())&#10;            .parameter(ToolParameter.builder()&#10;                .name(&quot;taskType&quot;)&#10;                .type(&quot;string&quot;)&#10;                .description(&quot;Type of task (debugging, refactoring, feature, review)&quot;)&#10;                .required(false)&#10;                .build())&#10;            .parameter(ToolParameter.builder()&#10;                .name(&quot;profile&quot;)&#10;                .type(&quot;string&quot;)&#10;                .description(&quot;Optimization profile (AGGRESSIVE, BALANCED, CONSERVATIVE)&quot;)&#10;                .required(false)&#10;                .defaultValue(&quot;BALANCED&quot;)&#10;                .build())&#10;            .build();&#10;    }&#10;    &#10;    @Override&#10;    public CompletableFuture&lt;Object&gt; execute(Map&lt;String, Object&gt; arguments) {&#10;        String filePath = (String) arguments.get(&quot;filePath&quot;);&#10;        String taskType = (String) arguments.get(&quot;taskType&quot;);&#10;        String profileStr = (String) arguments.getOrDefault(&quot;profile&quot;, &quot;BALANCED&quot;);&#10;        &#10;        OptimizationProfile profile = OptimizationProfile.valueOf(profileStr);&#10;        &#10;        return adkService.optimizeContext(filePath, taskType, profile)&#10;            .thenApply(result -&gt; Map.of(&#10;                &quot;filePath&quot;, result.getFilePath(),&#10;                &quot;optimizedContent&quot;, result.getOptimizedContent(),&#10;                &quot;originalTokens&quot;, result.getOriginalTokens(),&#10;                &quot;optimizedTokens&quot;, result.getOptimizedTokens(),&#10;                &quot;reductionRatio&quot;, result.getReductionRatio(),&#10;                &quot;confidence&quot;, result.getConfidence(),&#10;                &quot;modelUsed&quot;, result.getModelUsed()&#10;            ));&#10;    }&#10;}" />
              <option name="updatedContent" value="package com.contextengine.mcp.tools;&#10;&#10;import com.contextengine.mcp.adk.GoogleADKService;&#10;import com.contextengine.mcp.adk.OptimizationProfile;&#10;import com.contextengine.mcp.sdk.Tool;&#10;import com.contextengine.mcp.sdk.ToolDefinition;&#10;import com.contextengine.mcp.sdk.ToolParameter;&#10;import org.springframework.stereotype.Component;&#10;import org.slf4j.Logger;&#10;import org.slf4j.LoggerFactory;&#10;&#10;import java.util.Map;&#10;import java.util.concurrent.CompletableFuture;&#10;&#10;/**&#10; * MCP Tool: Optimize file context using Google ADK models&#10; */&#10;@Component&#10;public class OptimizeContextTool implements Tool {&#10;    private static final Logger logger = LoggerFactory.getLogger(OptimizeContextTool.class);&#10;    &#10;    private final GoogleADKService adkService;&#10;    &#10;    public OptimizeContextTool(GoogleADKService adkService) {&#10;        this.adkService = adkService;&#10;    }&#10;    &#10;    @Override&#10;    public ToolDefinition getDefinition() {&#10;        return ToolDefinition.builder()&#10;            .name(&quot;optimize_context&quot;)&#10;            .description(&quot;Optimize file context by removing boilerplate and keeping relevant code&quot;)&#10;            .parameter(ToolParameter.builder()&#10;                .name(&quot;filePath&quot;)&#10;                .type(&quot;string&quot;)&#10;                .description(&quot;Path to the file to optimize&quot;)&#10;                .required(true)&#10;                .build())&#10;            .parameter(ToolParameter.builder()&#10;                .name(&quot;taskType&quot;)&#10;                .type(&quot;string&quot;)&#10;                .description(&quot;Type of task (debugging, refactoring, feature, review)&quot;)&#10;                .required(false)&#10;                .build())&#10;            .parameter(ToolParameter.builder()&#10;                .name(&quot;profile&quot;)&#10;                .type(&quot;string&quot;)&#10;                .description(&quot;Optimization profile (AGGRESSIVE, BALANCED, CONSERVATIVE)&quot;)&#10;                .required(false)&#10;                .defaultValue(&quot;BALANCED&quot;)&#10;                .build())&#10;            .build();&#10;    }&#10;    &#10;    /**&#10;     * Executes file context optimization using Google ADK models.&#10;     * @param arguments Map of arguments (filePath, taskType, profile)&#10;     * @return CompletableFuture with optimization result&#10;     */&#10;    @Override&#10;    public CompletableFuture&lt;Object&gt; execute(Map&lt;String, Object&gt; arguments) {&#10;        String filePath = (String) arguments.get(&quot;filePath&quot;);&#10;        String taskType = (String) arguments.get(&quot;taskType&quot;);&#10;        String profileStr = (String) arguments.getOrDefault(&quot;profile&quot;, &quot;BALANCED&quot;);&#10;        if (filePath == null || filePath.isEmpty()) {&#10;            logger.warn(&quot;Missing required argument: filePath&quot;);&#10;            return CompletableFuture.failedFuture(new IllegalArgumentException(&quot;filePath is required&quot;));&#10;        }&#10;        OptimizationProfile profile;&#10;        try {&#10;            profile = OptimizationProfile.valueOf(profileStr);&#10;        } catch (Exception e) {&#10;            logger.warn(&quot;Invalid profile value: '{}', defaulting to BALANCED&quot;, profileStr);&#10;            profile = OptimizationProfile.BALANCED;&#10;        }&#10;        logger.info(&quot;Optimizing context for file: '{}', taskType: '{}', profile: '{}'&quot;, filePath, taskType, profile);&#10;        return adkService.optimizeContext(filePath, taskType, profile)&#10;            .thenApply(result -&gt; Map.of(&#10;                &quot;filePath&quot;, result.getFilePath(),&#10;                &quot;optimizedContent&quot;, result.getOptimizedContent(),&#10;                &quot;originalTokens&quot;, result.getOriginalTokens(),&#10;                &quot;optimizedTokens&quot;, result.getOptimizedTokens(),&#10;                &quot;reductionRatio&quot;, result.getReductionRatio(),&#10;                &quot;confidence&quot;, result.getConfidence(),&#10;                &quot;modelUsed&quot;, result.getModelUsed()&#10;            ))&#10;            .exceptionally(ex -&gt; {&#10;                logger.error(&quot;Context optimization failed: {}&quot;, ex.getMessage(), ex);&#10;                throw new RuntimeException(&quot;Context optimization failed&quot;, ex);&#10;            });&#10;    }&#10;}" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/context-engine-mcp/src/main/java/com/contextengine/mcp/tools/SearchCodeTool.java">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/context-engine-mcp/src/main/java/com/contextengine/mcp/tools/SearchCodeTool.java" />
              <option name="originalContent" value="package com.contextengine.mcp.tools;&#10;&#10;import com.contextengine.mcp.adk.GoogleADKService;&#10;import com.contextengine.mcp.adk.SearchOptions;&#10;import com.contextengine.mcp.adk.SearchResult;&#10;import com.contextengine.mcp.sdk.Tool;&#10;import com.contextengine.mcp.sdk.ToolDefinition;&#10;import com.contextengine.mcp.sdk.ToolParameter;&#10;import org.springframework.stereotype.Component;&#10;&#10;import java.util.Map;&#10;import java.util.concurrent.CompletableFuture;&#10;&#10;/**&#10; * MCP Tool: Semantic code search using Google ADK Vector Search&#10; */&#10;@Component&#10;public class SearchCodeTool implements Tool {&#10;    &#10;    private final GoogleADKService adkService;&#10;    &#10;    public SearchCodeTool(GoogleADKService adkService) {&#10;        this.adkService = adkService;&#10;    }&#10;    &#10;    @Override&#10;    public ToolDefinition getDefinition() {&#10;        return ToolDefinition.builder()&#10;            .name(&quot;search_code_semantic&quot;)&#10;            .description(&quot;Search code using semantic understanding powered by Google ADK&quot;)&#10;            .parameter(ToolParameter.builder()&#10;                .name(&quot;query&quot;)&#10;                .type(&quot;string&quot;)&#10;                .description(&quot;The search query&quot;)&#10;                .required(true)&#10;                .build())&#10;            .parameter(ToolParameter.builder()&#10;                .name(&quot;projectId&quot;)&#10;                .type(&quot;string&quot;)&#10;                .description(&quot;The project ID to search in&quot;)&#10;                .required(true)&#10;                .build())&#10;            .parameter(ToolParameter.builder()&#10;                .name(&quot;limit&quot;)&#10;                .type(&quot;integer&quot;)&#10;                .description(&quot;Maximum number of results&quot;)&#10;                .required(false)&#10;                .defaultValue(10)&#10;                .build())&#10;            .parameter(ToolParameter.builder()&#10;                .name(&quot;minScore&quot;)&#10;                .type(&quot;number&quot;)&#10;                .description(&quot;Minimum relevance score (0-1)&quot;)&#10;                .required(false)&#10;                .defaultValue(0.7)&#10;                .build())&#10;            .build();&#10;    }&#10;    &#10;    @Override&#10;    public CompletableFuture&lt;Object&gt; execute(Map&lt;String, Object&gt; arguments) {&#10;        String query = (String) arguments.get(&quot;query&quot;);&#10;        String projectId = (String) arguments.get(&quot;projectId&quot;);&#10;        int limit = (Integer) arguments.getOrDefault(&quot;limit&quot;, 10);&#10;        double minScore = (Double) arguments.getOrDefault(&quot;minScore&quot;, 0.7);&#10;        &#10;        SearchOptions options = new SearchOptions(limit, minScore, Map.of());&#10;        &#10;        return adkService.searchCode(query, projectId, options)&#10;            .thenApply(result -&gt; Map.of(&#10;                &quot;results&quot;, result.getResults(),&#10;                &quot;query&quot;, result.getQuery(),&#10;                &quot;modelUsed&quot;, result.getModelUsed(),&#10;                &quot;resultCount&quot;, result.getResults().size()&#10;            ));&#10;    }&#10;}" />
              <option name="updatedContent" value="package com.contextengine.mcp.tools;&#10;&#10;import com.contextengine.mcp.adk.GoogleADKService;&#10;import com.contextengine.mcp.adk.SearchOptions;&#10;import com.contextengine.mcp.adk.SearchResult;&#10;import com.contextengine.mcp.sdk.Tool;&#10;import com.contextengine.mcp.sdk.ToolDefinition;&#10;import com.contextengine.mcp.sdk.ToolParameter;&#10;import org.springframework.stereotype.Component;&#10;import org.slf4j.Logger;&#10;import org.slf4j.LoggerFactory;&#10;&#10;import java.util.Map;&#10;import java.util.concurrent.CompletableFuture;&#10;&#10;/**&#10; * MCP Tool: Semantic code search using Google ADK Vector Search&#10; */&#10;@Component&#10;public class SearchCodeTool implements Tool {&#10;    private static final Logger logger = LoggerFactory.getLogger(SearchCodeTool.class);&#10;    &#10;    private final GoogleADKService adkService;&#10;    &#10;    public SearchCodeTool(GoogleADKService adkService) {&#10;        this.adkService = adkService;&#10;    }&#10;    &#10;    @Override&#10;    public ToolDefinition getDefinition() {&#10;        return ToolDefinition.builder()&#10;            .name(&quot;search_code_semantic&quot;)&#10;            .description(&quot;Search code using semantic understanding powered by Google ADK&quot;)&#10;            .parameter(ToolParameter.builder()&#10;                .name(&quot;query&quot;)&#10;                .type(&quot;string&quot;)&#10;                .description(&quot;The search query&quot;)&#10;                .required(true)&#10;                .build())&#10;            .parameter(ToolParameter.builder()&#10;                .name(&quot;projectId&quot;)&#10;                .type(&quot;string&quot;)&#10;                .description(&quot;The project ID to search in&quot;)&#10;                .required(true)&#10;                .build())&#10;            .parameter(ToolParameter.builder()&#10;                .name(&quot;limit&quot;)&#10;                .type(&quot;integer&quot;)&#10;                .description(&quot;Maximum number of results&quot;)&#10;                .required(false)&#10;                .defaultValue(10)&#10;                .build())&#10;            .parameter(ToolParameter.builder()&#10;                .name(&quot;minScore&quot;)&#10;                .type(&quot;number&quot;)&#10;                .description(&quot;Minimum relevance score (0-1)&quot;)&#10;                .required(false)&#10;                .defaultValue(0.7)&#10;                .build())&#10;            .build();&#10;    }&#10;    &#10;    /**&#10;     * Executes a semantic code search using Google ADK Vector Search.&#10;     * @param arguments Map of arguments (query, projectId, limit, minScore)&#10;     * @return CompletableFuture with search results&#10;     */&#10;    @Override&#10;    public CompletableFuture&lt;Object&gt; execute(Map&lt;String, Object&gt; arguments) {&#10;        String query = (String) arguments.get(&quot;query&quot;);&#10;        String projectId = (String) arguments.get(&quot;projectId&quot;);&#10;        int limit = (Integer) arguments.getOrDefault(&quot;limit&quot;, 10);&#10;        double minScore = (Double) arguments.getOrDefault(&quot;minScore&quot;, 0.7);&#10;        if (query == null || query.isEmpty() || projectId == null || projectId.isEmpty()) {&#10;            logger.warn(&quot;Missing required arguments: query='{}', projectId='{}'&quot;, query, projectId);&#10;            return CompletableFuture.failedFuture(new IllegalArgumentException(&quot;query and projectId are required&quot;));&#10;        }&#10;        SearchOptions options = new SearchOptions(limit, minScore, Map.of());&#10;        logger.info(&quot;Executing semantic code search: query='{}', projectId='{}', limit={}, minScore={}&quot;, query, projectId, limit, minScore);&#10;        return adkService.searchCode(query, projectId, options)&#10;            .thenApply(result -&gt; Map.of(&#10;                &quot;results&quot;, result.getResults(),&#10;                &quot;query&quot;, result.getQuery(),&#10;                &quot;modelUsed&quot;, result.getModelUsed(),&#10;                &quot;resultCount&quot;, result.getResults().size()&#10;            ))&#10;            .exceptionally(ex -&gt; {&#10;                logger.error(&quot;Code search failed: {}&quot;, ex.getMessage(), ex);&#10;                throw new RuntimeException(&quot;Code search failed&quot;, ex);&#10;            });&#10;    }&#10;}" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/context-engine-mcp/src/main/java/com/contextengine/mcp/tools/SwitchModelTool.java">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/context-engine-mcp/src/main/java/com/contextengine/mcp/tools/SwitchModelTool.java" />
              <option name="originalContent" value="package com.contextengine.mcp.tools;&#10;&#10;import com.contextengine.mcp.llm.LLMRouter;&#10;import com.contextengine.mcp.llm.ModelRegistry;&#10;import com.contextengine.mcp.sdk.Tool;&#10;import com.contextengine.mcp.sdk.ToolDefinition;&#10;import com.contextengine.mcp.sdk.ToolParameter;&#10;import org.springframework.stereotype.Component;&#10;&#10;import java.util.Map;&#10;import java.util.concurrent.CompletableFuture;&#10;&#10;/**&#10; * MCP Tool: Switch between different LLM models at runtime&#10; */&#10;@Component&#10;public class SwitchModelTool implements Tool {&#10;    &#10;    private final LLMRouter router;&#10;    private final ModelRegistry registry;&#10;    &#10;    public SwitchModelTool(LLMRouter router, ModelRegistry registry) {&#10;        this.router = router;&#10;        this.registry = registry;&#10;    }&#10;    &#10;    @Override&#10;    public ToolDefinition getDefinition() {&#10;        return ToolDefinition.builder()&#10;            .name(&quot;switch_model&quot;)&#10;            .description(&quot;Switch the LLM model used for a specific operation&quot;)&#10;            .parameter(ToolParameter.builder()&#10;                .name(&quot;operation&quot;)&#10;                .type(&quot;string&quot;)&#10;                .description(&quot;Operation to configure (search, optimize, index, analyze, generate)&quot;)&#10;                .required(true)&#10;                .build())&#10;            .parameter(ToolParameter.builder()&#10;                .name(&quot;modelId&quot;)&#10;                .type(&quot;string&quot;)&#10;                .description(&quot;Model to use (gemini-flash, gemini-pro, gemini-nano, claude-opus, claude-sonnet, claude-haiku)&quot;)&#10;                .required(true)&#10;                .build())&#10;            .parameter(ToolParameter.builder()&#10;                .name(&quot;persist&quot;)&#10;                .type(&quot;boolean&quot;)&#10;                .description(&quot;Whether to persist this preference&quot;)&#10;                .required(false)&#10;                .defaultValue(true)&#10;                .build())&#10;            .build();&#10;    }&#10;    &#10;    @Override&#10;    public CompletableFuture&lt;Object&gt; execute(Map&lt;String, Object&gt; arguments) {&#10;        return CompletableFuture.supplyAsync(() -&gt; {&#10;            String operation = (String) arguments.get(&quot;operation&quot;);&#10;            String modelId = (String) arguments.get(&quot;modelId&quot;);&#10;            boolean persist = (Boolean) arguments.getOrDefault(&quot;persist&quot;, true);&#10;            &#10;            // Validate model exists&#10;            if (registry.getModel(modelId).isEmpty()) {&#10;                return Map.of(&#10;                    &quot;success&quot;, false,&#10;                    &quot;error&quot;, &quot;Model not found: &quot; + modelId,&#10;                    &quot;availableModels&quot;, registry.getAllModels().stream()&#10;                        .map(p -&gt; p.getId())&#10;                        .toList()&#10;                );&#10;            }&#10;            &#10;            // Switch model&#10;            router.setModelForOperation(operation, modelId);&#10;            &#10;            // TODO: Persist to database if persist=true&#10;            &#10;            return Map.of(&#10;                &quot;success&quot;, true,&#10;                &quot;operation&quot;, operation,&#10;                &quot;modelId&quot;, modelId,&#10;                &quot;persisted&quot;, persist,&#10;                &quot;currentAssignments&quot;, router.getModelAssignments()&#10;            );&#10;        });&#10;    }&#10;}" />
              <option name="updatedContent" value="package com.contextengine.mcp.tools;&#10;&#10;import com.contextengine.mcp.llm.LLMRouter;&#10;import com.contextengine.mcp.llm.ModelRegistry;&#10;import com.contextengine.mcp.sdk.Tool;&#10;import com.contextengine.mcp.sdk.ToolDefinition;&#10;import com.contextengine.mcp.sdk.ToolParameter;&#10;import org.springframework.stereotype.Component;&#10;import org.slf4j.Logger;&#10;import org.slf4j.LoggerFactory;&#10;&#10;import java.util.Map;&#10;import java.util.concurrent.CompletableFuture;&#10;&#10;/**&#10; * MCP Tool: Switch between different LLM models at runtime&#10; */&#10;@Component&#10;public class SwitchModelTool implements Tool {&#10;    private static final Logger logger = LoggerFactory.getLogger(SwitchModelTool.class);&#10;    &#10;    private final LLMRouter router;&#10;    private final ModelRegistry registry;&#10;    &#10;    public SwitchModelTool(LLMRouter router, ModelRegistry registry) {&#10;        this.router = router;&#10;        this.registry = registry;&#10;    }&#10;    &#10;    @Override&#10;    public ToolDefinition getDefinition() {&#10;        return ToolDefinition.builder()&#10;            .name(&quot;switch_model&quot;)&#10;            .description(&quot;Switch the LLM model used for a specific operation&quot;)&#10;            .parameter(ToolParameter.builder()&#10;                .name(&quot;operation&quot;)&#10;                .type(&quot;string&quot;)&#10;                .description(&quot;Operation to configure (search, optimize, index, analyze, generate)&quot;)&#10;                .required(true)&#10;                .build())&#10;            .parameter(ToolParameter.builder()&#10;                .name(&quot;modelId&quot;)&#10;                .type(&quot;string&quot;)&#10;                .description(&quot;Model to use (gemini-flash, gemini-pro, gemini-nano, claude-opus, claude-sonnet, claude-haiku)&quot;)&#10;                .required(true)&#10;                .build())&#10;            .parameter(ToolParameter.builder()&#10;                .name(&quot;persist&quot;)&#10;                .type(&quot;boolean&quot;)&#10;                .description(&quot;Whether to persist this preference&quot;)&#10;                .required(false)&#10;                .defaultValue(true)&#10;                .build())&#10;            .build();&#10;    }&#10;    &#10;    /**&#10;     * Executes a model switch for a specific operation.&#10;     * @param arguments Map of arguments (operation, modelId, persist)&#10;     * @return CompletableFuture with result of the switch&#10;     */&#10;    @Override&#10;    public CompletableFuture&lt;Object&gt; execute(Map&lt;String, Object&gt; arguments) {&#10;        return CompletableFuture.supplyAsync(() -&gt; {&#10;            String operation = (String) arguments.get(&quot;operation&quot;);&#10;            String modelId = (String) arguments.get(&quot;modelId&quot;);&#10;            boolean persist = (Boolean) arguments.getOrDefault(&quot;persist&quot;, true);&#10;            if (operation == null || operation.isEmpty() || modelId == null || modelId.isEmpty()) {&#10;                logger.warn(&quot;Missing required arguments: operation='{}', modelId='{}'&quot;, operation, modelId);&#10;                return Map.of(&#10;                    &quot;success&quot;, false,&#10;                    &quot;error&quot;, &quot;operation and modelId are required&quot;&#10;                );&#10;            }&#10;            // Validate model exists&#10;            if (registry.getModel(modelId).isEmpty()) {&#10;                logger.warn(&quot;Model not found: {}&quot;, modelId);&#10;                return Map.of(&#10;                    &quot;success&quot;, false,&#10;                    &quot;error&quot;, &quot;Model not found: &quot; + modelId,&#10;                    &quot;availableModels&quot;, registry.getAllModels().stream()&#10;                        .map(p -&gt; p.getId())&#10;                        .toList()&#10;                );&#10;            }&#10;            try {&#10;                router.setModelForOperation(operation, modelId);&#10;                logger.info(&quot;Switched model for operation '{}' to '{}' (persist={})&quot;, operation, modelId, persist);&#10;                // TODO: Persist to database if persist=true&#10;                return Map.of(&#10;                    &quot;success&quot;, true,&#10;                    &quot;operation&quot;, operation,&#10;                    &quot;modelId&quot;, modelId,&#10;                    &quot;persisted&quot;, persist,&#10;                    &quot;currentAssignments&quot;, router.getModelAssignments()&#10;                );&#10;            } catch (Exception e) {&#10;                logger.error(&quot;Failed to switch model: {}&quot;, e.getMessage(), e);&#10;                return Map.of(&#10;                    &quot;success&quot;, false,&#10;                    &quot;error&quot;, e.getMessage()&#10;                );&#10;            }&#10;        });&#10;    }&#10;}" />
            </PendingDiffInfo>
          </value>
        </entry>
      </map>
    </option>
  </component>
</project>