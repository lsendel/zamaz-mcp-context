# ðŸš€ Production-Ready LLM Integration Plan

## Overview
This document outlines a comprehensive plan to integrate real LLMs (Google Gemini) into the Zamaz MCP system using best practices, proper authentication, and environment-based configuration.

## Current Issues
1. Version conflicts between Google Cloud libraries
2. Hardcoded responses in the chat interface
3. Authentication not properly configured
4. No proper error handling for API failures
5. Missing environment-based configuration

## Implementation Plan

### Phase 1: Proper Google Cloud SDK Integration

#### 1.1 Use Google Cloud AI Platform Client Library
Instead of raw HTTP calls, use the official Google Cloud client library:

```java
@Service
public class GeminiLLMService {
    private final VertexAI vertexAI;
    private final GenerativeModel model;
    
    @Autowired
    public GeminiLLMService(
        @Value("${gcp.project-id}") String projectId,
        @Value("${gcp.location}") String location,
        @Value("${gcp.model.name}") String modelName
    ) {
        this.vertexAI = new VertexAI(projectId, location);
        this.model = new GenerativeModel(modelName, vertexAI);
    }
}
```

#### 1.2 Environment-Based Configuration
Create profiles for different environments:

**application.yml**
```yaml
spring:
  profiles:
    active: ${SPRING_PROFILE:local}

---
spring:
  config:
    activate:
      on-profile: local
gcp:
  project-id: ${GCP_PROJECT_ID}
  location: ${GCP_LOCATION:us-central1}
  model:
    name: ${GCP_MODEL:gemini-1.5-flash}

---
spring:
  config:
    activate:
      on-profile: production
gcp:
  project-id: ${GCP_PROJECT_ID}
  location: ${GCP_LOCATION:us-central1}
  model:
    name: ${GCP_MODEL:gemini-1.5-pro}
```

### Phase 2: Authentication Best Practices

#### 2.1 Service Account Authentication
For production use:
```bash
# Create service account
gcloud iam service-accounts create zamaz-mcp-llm \
    --display-name="Zamaz MCP LLM Service"

# Grant necessary roles
gcloud projects add-iam-policy-binding ${PROJECT_ID} \
    --member="serviceAccount:zamaz-mcp-llm@${PROJECT_ID}.iam.gserviceaccount.com" \
    --role="roles/aiplatform.user"

# Download key
gcloud iam service-accounts keys create service-account-key.json \
    --iam-account=zamaz-mcp-llm@${PROJECT_ID}.iam.gserviceaccount.com
```

#### 2.2 Workload Identity for GKE
If running on GKE:
```yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: zamaz-mcp
  annotations:
    iam.gke.io/gcp-service-account: zamaz-mcp-llm@PROJECT_ID.iam.gserviceaccount.com
```

### Phase 3: Robust LLM Service Implementation

#### 3.1 Service Interface
```java
public interface LLMService {
    CompletableFuture<String> generateResponse(String prompt, LLMConfig config);
    CompletableFuture<ChatResponse> chat(ChatRequest request);
    boolean testConnection();
}
```

#### 3.2 Implementation with Retry and Circuit Breaker
```java
@Service
@Slf4j
public class GeminiLLMService implements LLMService {
    
    @Retry(name = "gemini-api", fallbackMethod = "fallbackResponse")
    @CircuitBreaker(name = "gemini-api")
    public CompletableFuture<String> generateResponse(String prompt, LLMConfig config) {
        return CompletableFuture.supplyAsync(() -> {
            try {
                GenerativeModel model = getModel(config);
                GenerateContentResponse response = model.generateContent(prompt);
                return ResponseHandler.getText(response);
            } catch (Exception e) {
                throw new LLMServiceException("Failed to generate response", e);
            }
        });
    }
}
```

### Phase 4: Chat Console Integration

#### 4.1 WebSocket Support for Real-time Chat
```java
@Configuration
@EnableWebSocket
public class WebSocketConfig implements WebSocketConfigurer {
    @Override
    public void registerWebSocketHandlers(WebSocketHandlerRegistry registry) {
        registry.addHandler(new ChatWebSocketHandler(), "/ws/chat")
                .setAllowedOrigins("*");
    }
}
```

#### 4.2 Enhanced Chat Controller
```java
@RestController
@RequestMapping("/api/v1/chat")
public class ChatController {
    
    @PostMapping
    public Mono<ChatResponse> chat(@RequestBody @Valid ChatRequest request) {
        return llmService.chat(request)
            .map(response -> ChatResponse.builder()
                .message(response)
                .agent(request.getAgent())
                .timestamp(Instant.now())
                .build()
            );
    }
}
```

### Phase 5: Error Handling and Monitoring

#### 5.1 Global Exception Handler
```java
@RestControllerAdvice
public class GlobalExceptionHandler {
    
    @ExceptionHandler(LLMServiceException.class)
    public ResponseEntity<ErrorResponse> handleLLMError(LLMServiceException e) {
        return ResponseEntity.status(HttpStatus.SERVICE_UNAVAILABLE)
            .body(ErrorResponse.of("LLM service temporarily unavailable"));
    }
}
```

#### 5.2 Health Checks
```java
@Component
public class LLMHealthIndicator implements HealthIndicator {
    
    @Override
    public Health health() {
        try {
            boolean connected = llmService.testConnection();
            return connected ? Health.up().build() : Health.down().build();
        } catch (Exception e) {
            return Health.down(e).build();
        }
    }
}
```

### Phase 6: Testing Strategy

#### 6.1 Unit Tests with Mocks
```java
@ExtendWith(MockitoExtension.class)
class LLMServiceTest {
    @Mock
    private VertexAI vertexAI;
    
    @Test
    void shouldGenerateResponse() {
        // Test implementation
    }
}
```

#### 6.2 Integration Tests
```java
@SpringBootTest
@TestPropertySource(properties = {
    "gcp.project-id=test-project",
    "gcp.location=us-central1"
})
class LLMIntegrationTest {
    // Integration tests
}
```

### Phase 7: Deployment Configuration

#### 7.1 Docker Configuration
```dockerfile
FROM openjdk:17-jdk-slim
COPY target/context-engine-mcp.jar app.jar
ENV GOOGLE_APPLICATION_CREDENTIALS=/app/credentials/service-account.json
ENTRYPOINT ["java", "-jar", "/app.jar"]
```

#### 7.2 Kubernetes Deployment
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: zamaz-mcp
spec:
  template:
    spec:
      containers:
      - name: app
        env:
        - name: GCP_PROJECT_ID
          valueFrom:
            configMapKeyRef:
              name: app-config
              key: gcp.project-id
```

## Implementation Steps

1. **Week 1**: Set up proper Google Cloud SDK integration
   - Remove version conflicts
   - Implement proper client initialization
   - Add retry and circuit breaker patterns

2. **Week 2**: Authentication and Security
   - Set up service accounts
   - Implement credential management
   - Add API key rotation

3. **Week 3**: Enhanced Chat Interface
   - WebSocket support
   - Real-time streaming responses
   - Multi-turn conversations

4. **Week 4**: Testing and Deployment
   - Comprehensive test suite
   - Performance testing
   - Production deployment

## Success Metrics

- âœ… Zero hardcoded responses
- âœ… 99.9% uptime for LLM service
- âœ… < 2s response time for chat queries
- âœ… Proper error handling and fallbacks
- âœ… Environment-based configuration
- âœ… Comprehensive monitoring and alerts

## Next Steps

1. Review and approve this plan
2. Set up development environment with proper credentials
3. Begin Phase 1 implementation
4. Weekly progress reviews